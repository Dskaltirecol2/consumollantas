[
  {
    "objectID": "eda_final.html",
    "href": "eda_final.html",
    "title": "Informe",
    "section": "",
    "text": "DOCUMENTO: INFORME ANALISIS CONSUMO DE LLANTAS CERREJON\n    CLIENTE: CERREJON COLOMBIA\n    ULTIMA VERSION: Rev. 01\n    REALIZADO POR: √Årea inteligencia de Datos KAL TIRE\n    FECHA: Octubre 2, 2025"
  },
  {
    "objectID": "eda_final.html#problema-de-negocio",
    "href": "eda_final.html#problema-de-negocio",
    "title": "Informe",
    "section": "1.1 Problema de Negocio",
    "text": "1.1 Problema de Negocio\nSin pron√≥stico:\n\nCompras reactivas cuando se agota inventario\nSobrestock por precauci√≥n (capital inmovilizado)\nParos operacionales por desabastecimiento\n\nCon pron√≥stico:\n\nCompras planificadas\nReducci√≥n de inventario\nNegociaci√≥n anticipada con proveedores\nPrevenci√≥n de paros operacionales"
  },
  {
    "objectID": "eda_final.html#enfoque-dos-modelos-complementarios",
    "href": "eda_final.html#enfoque-dos-modelos-complementarios",
    "title": "Informe",
    "section": "1.2 Enfoque: Dos Modelos Complementarios",
    "text": "1.2 Enfoque: Dos Modelos Complementarios\n\n1.2.1 Modelo Autorregresivo\nQu√© usa: Solo historial de consumo pasado (lags, medias m√≥viles, tendencias)\nCu√°ndo usar:\n\nPron√≥sticos de emergencia\nPlan minero incierto\nBaseline de comparaci√≥n\n\nVentajas: Simple, robusto, no requiere datos externos\nDesventajas: No captura causas operacionales\n\n\n1.2.2 Modelo con Variables Ex√≥genas\nQu√© usa: Factores operacionales que causan el consumo\nVariables principales:\n\nPlan minero (lag 0): km programados, ciclos, carga √∫til\nMantenimientos (lag 1-4): preventivos/correctivos\nInspecciones (lag 3-4): hallazgos cr√≠ticos\nClima (lag 3): precipitaci√≥n\nDerivados: intensidad uso, payload/km, presi√≥n mantenimiento\n\nCu√°ndo usar:\n\nPlan minero disponible\nM√°xima precisi√≥n requerida\nAn√°lisis ‚Äúwhat-if‚Äù\n\nVentajas: Captura relaciones causales, m√°s preciso\nDesventajas: Requiere m√°s datos, m√°s complejo"
  },
  {
    "objectID": "eda_final.html#metodolog√≠a",
    "href": "eda_final.html#metodolog√≠a",
    "title": "Informe",
    "section": "1.3 Metodolog√≠a",
    "text": "1.3 Metodolog√≠a\nDatos: 540 datos diarios (2024-02-07 ‚Üí 2025-10-09)\nSplit: Train (80) ‚Üí Val (10) ‚Üí Test (10)\nValidaci√≥n: Time Series CV (5 folds)\nOptimizaci√≥n: Optuna (80-100 trials por modelo)\nModelos evaluados: 11 algoritmos\n\nLineales: Ridge, Lasso, ElasticNet\nBoosting: LightGBM, XGBoost, CatBoost\nEnsemble: Stacking de top 5\n\nM√©trica principal: sMAPE (error porcentual sim√©trico)"
  },
  {
    "objectID": "eda_final.html#valor-entregado",
    "href": "eda_final.html#valor-entregado",
    "title": "Informe",
    "section": "1.4 Valor Entregado",
    "text": "1.4 Valor Entregado\nOperacional:\n\nPron√≥stico 2-6 semanas adelante\nIntervalos de confianza\nAlertas autom√°ticas\n\nFinanciero:\n\nReducci√≥n inventario: 15-25%\nAhorro en compras: 5-10%\nPrevenci√≥n paros: Valor alto\n\nGesti√≥n:\n\nKPIs confiables\nJustificaci√≥n presupuestal\nCoordinaci√≥n con planificaci√≥n"
  },
  {
    "objectID": "eda_final.html#exploracion-temporal",
    "href": "eda_final.html#exploracion-temporal",
    "title": "Informe",
    "section": "2.1 Exploracion temporal",
    "text": "2.1 Exploracion temporal"
  },
  {
    "objectID": "eda_final.html#analisis-distribucional-y-de-outliers",
    "href": "eda_final.html#analisis-distribucional-y-de-outliers",
    "title": "Informe",
    "section": "2.2 Analisis distribucional y de outliers",
    "text": "2.2 Analisis distribucional y de outliers\n\n2.2.1 Analisis de distribucion\n\n2.2.1.1 Hist+KDE\n\n\n                            \n                                            \n\n\n\n\n2.2.1.2 Boxplots\n\n\n                            \n                                            \n\n\n\n\n\n2.2.2 Analisis de outliers\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\n\nNombre\nTotal_Datos\nOutliers\nPct_Outliers\n\n\n\n\n0\nLlantas Consumidas\n540\n4\n0.74\n\n\n1\nInspecciones Prio 1\n540\n17\n3.15\n\n\n2\nInspecciones Prio 2\n540\n15\n2.78\n\n\n3\nLlantas Desechadas\n540\n7\n1.30\n\n\n4\nMantenimiento Preventivo\n540\n4\n0.74\n\n\n5\nMantenimiento Correctivo\n540\n27\n5.00\n\n\n6\nVelocidad Vac√≠o (km/h)\n537\n23\n4.28\n\n\n7\nCarga Promedio (ton)\n537\n7\n1.30\n\n\n8\nHoras down diario\n540\n17\n3.15"
  },
  {
    "objectID": "eda_final.html#analisis-de-correlacion",
    "href": "eda_final.html#analisis-de-correlacion",
    "title": "Informe",
    "section": "2.3 Analisis de correlacion",
    "text": "2.3 Analisis de correlacion"
  },
  {
    "objectID": "eda_final.html#autocorrelacion-y-autocorrelacion-parcial",
    "href": "eda_final.html#autocorrelacion-y-autocorrelacion-parcial",
    "title": "Informe",
    "section": "2.4 Autocorrelacion y autocorrelacion parcial",
    "text": "2.4 Autocorrelacion y autocorrelacion parcial\n\n\n\n\n\n\n\n\n\n\n2.4.1 Analisis de resultados\n\nEstacionariedad: S√≠\nADF p-value: 0.0000\nLags significativos ACF:\n[‚ÄòLag 1: 0.1447‚Äô, ‚ÄòLag 6: 0.0851‚Äô, ‚ÄòLag 7: 0.1532‚Äô, ‚ÄòLag 8: 0.1372‚Äô, ‚ÄòLag 9: 0.1039‚Äô]\nLags significativos PACF:\n[‚ÄòLag 1: 0.1450‚Äô, ‚ÄòLag 7: 0.1357‚Äô, ‚ÄòLag 8: 0.0947‚Äô, ‚ÄòLag 19: 0.0970‚Äô]"
  },
  {
    "objectID": "eda_final.html#analisis-de-diferentes-granularidades-temporales",
    "href": "eda_final.html#analisis-de-diferentes-granularidades-temporales",
    "title": "Informe",
    "section": "2.5 Analisis de diferentes granularidades temporales",
    "text": "2.5 Analisis de diferentes granularidades temporales\n\n2.5.1 Histogramas de distribuci√≥n\n\n\n                            \n                                            \n\n\n\n\n2.5.2 Boxplots\n\n\n                            \n                                            \n\n\n\n\n2.5.3 Series temporales\n\n\n                            \n                                            \n\n\n\n\n2.5.4 Analisis de autocorrelacion ACF y PACF\n\n\n\nDiario:\n  Lags significativos en ACF: [1, 6, 7, 8, 9]\n  Lags significativos en PACF: [1, 7, 8]\n\nSemanal:\n  Lags significativos en ACF: [1, 2, 13]\n  Lags significativos en PACF: [1, 13]\n\nQuincenal:\n  Lags significativos en PACF: [15]\n\nMensual:\n  Lags significativos en PACF: [3, 5, 8]\n\n\n\n\n\n\n\n\n\n\n\n2.5.5 Analisis de correlacion y feature importance\n\n\n====================================================================================================\nAN√ÅLISIS DE LAGS √ìPTIMOS EN M√öLTIPLES FRECUENCIAS\n====================================================================================================\n\nDatasets generados:\n  Diario      :  540 per√≠odos | CV: 56.6% | Max lags: 30 d√≠as\n  Semanal     :   88 per√≠odos | CV: 36.9% | Max lags: 4 semanas\n  Quincenal   :   41 per√≠odos | CV: 31.1% | Max lags: 8 quincenas\n  Mensual     :   21 per√≠odos | CV: 35.7% | Max lags: 6 meses\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nüìä Creando lags autorregresivos (ACF/PACF)...\nüìä Creando rolling features del consumo...\nüìä Creando lags espec√≠ficos (cross-correlation)...\nüìä Creando rolling features de variables ex√≥genas...\nüìä Creando features de interacci√≥n...\nüìä Creando features de tendencia...\nüìä Limpiando NaN...\n‚úÖ Dataset final: 501 filas, 107 columnas\n   (se eliminaron 39 filas con NaN)\n\nüîç VERIFICANDO DATA LEAKAGE...\n‚ö†Ô∏è  ADVERTENCIA: Estas columnas originales est√°n en los features:\n   - total_loaded_kms\n   - total_empty_kms\n   - total_payload\n   - no_cycles\n   - total_cycle_time\n   - avg_empty_speed\n   - no_inspe_prio_1\n   - no_inspe_prio_2\n   - no_preventivos\n   - no_correctivos\n   - llantas_desechadas\n   - avg_payload\n   Considera eliminarlas si no se usan con lag/rolling\n\nüìä Evaluando todas las features...\n   Features a evaluar: 72\n\n==========================================================================================\nüìå TOP 30 FEATURES M√ÅS IMPORTANTES (ordenadas por Mutual Information)\n==========================================================================================\n\n\n\n\n\n\n\n\n\nPearson\nSpearman\nMutual_Info\nAbs_Pearson\n\n\n\n\nconsumo_mean_3\n0.1160\n0.1405\n0.0888\n0.1160\n\n\npresion_mantenimiento\n-0.0947\n-0.0844\n0.0814\n0.0947\n\n\nllantas_desechadas_lag12\n0.1338\n0.1204\n0.0738\n0.1338\n\n\nno_correctivos_sum14\n0.0777\n0.0689\n0.0726\n0.0777\n\n\npayload_per_km\n0.0167\n-0.0145\n0.0670\n0.0167\n\n\nintensidad_uso\n0.0971\n0.1154\n0.0644\n0.0971\n\n\ndow_sin\n0.0040\n-0.0182\n0.0636\n0.0040\n\n\nconsumo_lag7\n0.1587\n0.1372\n0.0634\n0.1587\n\n\ndow\n0.1066\n0.1238\n0.0542\n0.1066\n\n\nno_preventivos_sum7\n-0.0764\n-0.0690\n0.0537\n0.0764\n\n\nhoras_down\n0.1912\n0.1879\n0.0525\n0.1912\n\n\nno_inspe_prio_2_mean7\n-0.1772\n-0.1495\n0.0502\n0.1772\n\n\nno_cycles_std7\n0.0280\n0.0426\n0.0468\n0.0280\n\n\nconsumo_std_7\n0.1156\n0.1120\n0.0448\n0.1156\n\n\ntotal_empty_kms_lag10\n0.1168\n0.0823\n0.0445\n0.1168\n\n\nno_preventivos_lag15\n-0.1298\n-0.1131\n0.0435\n0.1298\n\n\nno_inspe_prio_1_lag28\n-0.0931\n-0.0867\n0.0428\n0.0931\n\n\nconsumo_lag1\n0.1586\n0.1701\n0.0404\n0.1586\n\n\nconsumo_lag19\n0.1052\n0.1166\n0.0401\n0.1052\n\n\ntotal_payload_mean7\n0.0334\n0.0131\n0.0385\n0.0334\n\n\ntotal_payload_lag10\n0.1077\n0.0718\n0.0345\n0.1077\n\n\ntotal_cycle_time_mean3\n0.0534\n0.0110\n0.0342\n0.0534\n\n\nno_inspe_prio_1_mean7\n-0.0499\n-0.0771\n0.0337\n0.0499\n\n\nno_preventivos_sum14\n-0.0631\n-0.0451\n0.0327\n0.0631\n\n\nconsumo_diff1\n0.0740\n0.0876\n0.0322\n0.0740\n\n\nconsumo_volatility_7\n-0.0038\n-0.0110\n0.0307\n0.0038\n\n\nconsumo_lag8\n0.1469\n0.1468\n0.0304\n0.1469\n\n\nno_correctivos_lag9\n-0.0921\n-0.0624\n0.0275\n0.0921\n\n\nno_inspe_prio_2_sum7\n-0.1772\n-0.1495\n0.0261\n0.1772\n\n\nconsumo_acceleration\n0.0168\n0.0276\n0.0244\n0.0168\n\n\n\n\n\n\n\n\n==========================================================================================\nüìä AN√ÅLISIS POR CATEGOR√çA DE FEATURES\n==========================================================================================\n\nüîπ Rolling:\n   ‚Ä¢ consumo_mean_3                                MI: 0.0888 | Pearson:  0.1160\n   ‚Ä¢ no_correctivos_sum14                          MI: 0.0726 | Pearson:  0.0777\n   ‚Ä¢ no_preventivos_sum7                           MI: 0.0537 | Pearson: -0.0764\n   ‚Ä¢ no_inspe_prio_2_mean7                         MI: 0.0502 | Pearson: -0.1772\n   ‚Ä¢ no_cycles_std7                                MI: 0.0468 | Pearson:  0.0280\n\nüîπ Interacci√≥n:\n   ‚Ä¢ presion_mantenimiento                         MI: 0.0814 | Pearson: -0.0947\n   ‚Ä¢ payload_per_km                                MI: 0.0670 | Pearson:  0.0167\n   ‚Ä¢ intensidad_uso                                MI: 0.0644 | Pearson:  0.0971\n   ‚Ä¢ ratio_hallazgos_mantenimiento                 MI: 0.0089 | Pearson: -0.1172\n   ‚Ä¢ ratio_cargado_vacio                           MI: 0.0000 | Pearson: -0.0006\n\nüîπ Lag Ex√≥geno:\n   ‚Ä¢ llantas_desechadas_lag12                      MI: 0.0738 | Pearson:  0.1338\n   ‚Ä¢ consumo_lag7                                  MI: 0.0634 | Pearson:  0.1587\n   ‚Ä¢ total_empty_kms_lag10                         MI: 0.0445 | Pearson:  0.1168\n   ‚Ä¢ no_preventivos_lag15                          MI: 0.0435 | Pearson: -0.1298\n   ‚Ä¢ no_inspe_prio_1_lag28                         MI: 0.0428 | Pearson: -0.0931\n\nüîπ Otra:\n   ‚Ä¢ dow_sin                                       MI: 0.0636 | Pearson:  0.0040\n   ‚Ä¢ dow                                           MI: 0.0542 | Pearson:  0.1066\n   ‚Ä¢ horas_down                                    MI: 0.0525 | Pearson:  0.1912\n   ‚Ä¢ dow_cos                                       MI: 0.0061 | Pearson: -0.2208\n   ‚Ä¢ mes                                           MI: 0.0000 | Pearson:  0.0057\n\nüîπ Tendencia:\n   ‚Ä¢ consumo_diff1                                 MI: 0.0322 | Pearson:  0.0740\n   ‚Ä¢ consumo_acceleration                          MI: 0.0244 | Pearson:  0.0168\n   ‚Ä¢ kms_loaded_trend                              MI: 0.0100 | Pearson:  0.0127\n   ‚Ä¢ consumo_diff7                                 MI: 0.0000 | Pearson:  0.0089\n   ‚Ä¢ consumo_trend                                 MI: 0.0000 | Pearson: -0.0015\n\nüîπ Autorregresivo:\n   ‚Ä¢ consumo_volatility_7                          MI: 0.0307 | Pearson: -0.0038\n   ‚Ä¢ consumo_max_7                                 MI: 0.0060 | Pearson:  0.1247\n   ‚Ä¢ consumo_min_7                                 MI: 0.0000 | Pearson:  0.0333\n\nüîπ Calendario:\n   ‚Ä¢ mes_sin                                       MI: 0.0000 | Pearson: -0.0151\n   ‚Ä¢ mes_cos                                       MI: 0.0000 | Pearson: -0.0787"
  },
  {
    "objectID": "eda_final.html#diario",
    "href": "eda_final.html#diario",
    "title": "Informe",
    "section": "3.1 Diario",
    "text": "3.1 Diario\n\n3.1.1 Baseline\n\n\n================================================================================\nBASELINE DE PRON√ìSTICO - MODELOS DE SERIES TEMPORALES\n================================================================================\n\nDatos: 611 observaciones\nRango: 2024-02-07 a 2025-10-09\nMedia: 4.03, Desv. Est: 2.84\n\nConfiguraci√≥n:\n  - Horizonte (H): 14 d√≠as\n  - Estacionalidad (m): 7 d√≠as\n  - Folds de validaci√≥n: 5\n\nSplits de validaci√≥n cruzada generados:\n  Split 1: Train hasta 2025-07-31, Test 2025-08-01 - 2025-08-14 (541 train / 14 test)\n  Split 2: Train hasta 2025-08-14, Test 2025-08-15 - 2025-08-28 (555 train / 14 test)\n  Split 3: Train hasta 2025-08-28, Test 2025-08-29 - 2025-09-11 (569 train / 14 test)\n  Split 4: Train hasta 2025-09-11, Test 2025-09-12 - 2025-09-25 (583 train / 14 test)\n  Split 5: Train hasta 2025-09-25, Test 2025-09-26 - 2025-10-09 (597 train / 14 test)\n\n================================================================================\nB√öSQUEDA DE MEJOR CONFIGURACI√ìN SARIMAX\n================================================================================\n\nProbando 55 configuraciones...\n\nMejor configuraci√≥n encontrada:\n  Order: (1, 0, 1)\n  Seasonal Order: (1, 1, 1, 7)\n  AIC: 2550.63\n\n================================================================================\nEVALUANDO MODELOS BASELINE CON TIME SERIES CV\n================================================================================\n\nEvaluando: Naive... sMAPE: 87.92%\n\nEvaluando: Seasonal_Naive... sMAPE: 89.81%\n\nEvaluando: MA_7... sMAPE: 69.30%\n\nEvaluando: MA_14... sMAPE: 70.24%\n\nEvaluando: SES... sMAPE: 69.23%\n\nEvaluando: Holt_Winters... sMAPE: 68.27%\n\nEvaluando: Theta... sMAPE: 67.72%\n\nEvaluando: SARIMAX... sMAPE: 68.79%\n\n================================================================================\nRESULTADOS FINALES - BASELINE\n================================================================================\n\nRanking por sMAPE (mean ¬± std):\n\n        Modelo  sMAPE_mean  sMAPE_std  MAE_mean  MAE_std  RMSE_mean  RMSE_std  MASE_mean  MASE_std\n         Theta   67.719109  18.012711  1.920849 0.431403   2.335429  0.436820   0.670644  0.149331\n  Holt_Winters   68.267463  17.817200  1.957290 0.438879   2.360904  0.446095   0.683339  0.151799\n       SARIMAX   68.785856  16.243888  1.985619 0.272111   2.372629  0.349833   0.693390  0.094570\n           SES   69.229132  13.781948  2.001697 0.468041   2.360312  0.561966   0.698316  0.159837\n          MA_7   69.302300  14.311414  1.948980 0.582840   2.277760  0.632428   0.680110  0.200925\n         MA_14   70.239308  15.056195  2.024490 0.530445   2.383537  0.641245   0.706393  0.182258\n         Naive   87.923790  40.879580  2.385714 0.436241   2.867619  0.523950   0.833459  0.154754\nSeasonal_Naive   89.806257  31.792944  2.385714 0.598894   2.909443  0.761341   0.833836  0.212075\n\nMEJOR MODELO: Theta\n  sMAPE: 67.72%\n  MAE: 1.92 ¬± 0.43\n  RMSE: 2.34 ¬± 0.44\n\nGenerando visualizaciones...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResultados guardados en 'baseline_resultados_cv.csv'\nPredicciones del mejor modelo guardadas en 'baseline_predicciones_Theta.csv'\n\n================================================================================\nBASELINE COMPLETADO\n================================================================================\n\nBenchmark a superar: sMAPE &lt; 67.72%\n\n\n\n\n3.1.2 Machine learning (Aprendizaje de maquina)\n\n\n================================================================================\nMODELOS DE MACHINE LEARNING - CON Y SIN EX√ìGENAS\n================================================================================\n\nDataset: 501 filas, 107 columnas\n\n================================================================================\nFEATURE SELECTION\n================================================================================\n\n1Ô∏è‚É£  ESCENARIO 1: SOLO FEATURES AUTORREGRESIVAS\n--------------------------------------------------------------------------------\nFeatures autorregresivas disponibles: 9\nDespu√©s de eliminar colinealidad: 9\n\nFeatures seleccionadas:\n  1. consumo_mean_3                           (MI: 0.0888)\n  2. consumo_lag7                             (MI: 0.0634)\n  3. consumo_std_7                            (MI: 0.0448)\n  4. consumo_lag1                             (MI: 0.0404)\n  5. consumo_lag19                            (MI: 0.0401)\n  6. consumo_diff1                            (MI: 0.0322)\n  7. consumo_volatility_7                     (MI: 0.0307)\n  8. consumo_lag8                             (MI: 0.0304)\n  9. consumo_acceleration                     (MI: 0.0244)\n\n2Ô∏è‚É£  ESCENARIO 2: CON VARIABLES EX√ìGENAS\n--------------------------------------------------------------------------------\nFeatures con MI &gt; 0.03: 27\nL√≠mite por tama√±o de muestra (n/35): 14\nDespu√©s de eliminar colinealidad: 14\n\nTop 15 features seleccionadas:\n  1. consumo_mean_3                           (MI: 0.0888)\n  2. presion_mantenimiento                    (MI: 0.0814)\n  3. llantas_desechadas_lag12                 (MI: 0.0738)\n  4. no_correctivos_sum14                     (MI: 0.0726)\n  5. payload_per_km                           (MI: 0.0670)\n  6. intensidad_uso                           (MI: 0.0644)\n  7. dow_sin                                  (MI: 0.0636)\n  8. consumo_lag7                             (MI: 0.0634)\n  9. dow                                      (MI: 0.0542)\n  10. no_preventivos_sum7                      (MI: 0.0537)\n  11. horas_down                               (MI: 0.0525)\n  12. no_inspe_prio_2_mean7                    (MI: 0.0502)\n  13. no_cycles_std7                           (MI: 0.0468)\n  14. consumo_std_7                            (MI: 0.0448)\n\n================================================================================\nPREPARACI√ìN PARA MODELADO\n================================================================================\n\nDataset autorregresivo: (501, 9)\nDataset con ex√≥genas: (501, 14)\n\nTime Series CV: 5 folds, test_size=14\n\n================================================================================\nCONFIGURACI√ìN DE MODELOS\n================================================================================\n\nModelos configurados:\n  - Ridge\n  - Lasso\n  - ElasticNet\n  - RandomForest\n  - LightGBM\n  - XGBoost\n\n================================================================================\nENTRENAMIENTO Y EVALUACI√ìN\n================================================================================\n\nüîÑ ESCENARIO 1: Solo features autorregresivas\n--------------------------------------------------------------------------------\n\nEvaluando Ridge... sMAPE: 46.13% ¬± 7.43%\n\nEvaluando Lasso... sMAPE: 46.96% ¬± 11.91%\n\nEvaluando ElasticNet... sMAPE: 46.96% ¬± 11.91%\n\nEvaluando RandomForest... sMAPE: 46.24% ¬± 7.72%\n\nEvaluando LightGBM... sMAPE: 46.53% ¬± 8.59%\n\nEvaluando XGBoost... sMAPE: 47.04% ¬± 8.01%\n\nüîÑ ESCENARIO 2: Con variables ex√≥genas\n--------------------------------------------------------------------------------\n\nEvaluando Ridge... sMAPE: 46.34% ¬± 9.35%\n\nEvaluando Lasso... sMAPE: 46.96% ¬± 11.91%\n\nEvaluando ElasticNet... sMAPE: 46.96% ¬± 11.91%\n\nEvaluando RandomForest... sMAPE: 46.53% ¬± 10.59%\n\nEvaluando LightGBM... sMAPE: 47.69% ¬± 14.58%\n\nEvaluando XGBoost... sMAPE: 47.33% ¬± 11.98%\n\n================================================================================\nRESULTADOS - MODELOS DE MACHINE LEARNING\n================================================================================\n\nRanking completo por sMAPE:\n\n      Modelo    Escenario  N_Features  sMAPE_mean  sMAPE_std  MAE_mean  MAE_std  RMSE_mean  RMSE_std\n       Ridge    Solo_Auto           9   46.134275   7.433865  1.913145 0.259716   2.205667  0.344344\nRandomForest    Solo_Auto           9   46.241335   7.719800  1.918602 0.254096   2.209172  0.298811\n       Ridge Con_Exogenas          14   46.340774   9.354052  1.915817 0.142864   2.228022  0.213109\n    LightGBM    Solo_Auto           9   46.529200   8.594646  1.946500 0.223505   2.262996  0.279563\nRandomForest Con_Exogenas          14   46.532502  10.588667  1.925234 0.166707   2.225878  0.246964\n  ElasticNet    Solo_Auto           9   46.956888  11.907370  1.957585 0.216664   2.290254  0.241639\n       Lasso Con_Exogenas          14   46.956888  11.907370  1.957585 0.216664   2.290254  0.241639\n       Lasso    Solo_Auto           9   46.956888  11.907370  1.957585 0.216664   2.290254  0.241639\n  ElasticNet Con_Exogenas          14   46.956888  11.907370  1.957585 0.216664   2.290254  0.241639\n     XGBoost    Solo_Auto           9   47.041851   8.014412  1.969199 0.279602   2.261264  0.340850\n     XGBoost Con_Exogenas          14   47.327048  11.975461  1.970409 0.241621   2.254910  0.282013\n    LightGBM Con_Exogenas          14   47.693186  14.575874  2.001328 0.385314   2.312630  0.396640\n\n--------------------------------------------------------------------------------\nCOMPARACI√ìN POR ESCENARIO\n--------------------------------------------------------------------------------\n\nSolo_Auto:\n  Mejor: Ridge - sMAPE: 46.13% ¬± 7.43%\n  Features: 9\n\nCon_Exogenas:\n  Mejor: Ridge - sMAPE: 46.34% ¬± 9.35%\n  Features: 14\n\nüèÜ MEJOR MODELO OVERALL:\n   Ridge (Solo_Auto)\n   sMAPE: 46.13% ¬± 7.43%\n   MAE: 1.91 ¬± 0.26\n   RMSE: 2.21 ¬± 0.34\n   Features: 9\n\n================================================================================\nFEATURE IMPORTANCE - MEJORES MODELOS\n================================================================================\n\nRidge - Solo_Auto\n------------------------------------------------------------\n             feature  importance\n        consumo_lag8    0.276967\n        consumo_lag7    0.268153\n        consumo_lag1    0.251163\n       consumo_lag19    0.243716\n       consumo_std_7    0.232491\nconsumo_volatility_7    0.219122\nconsumo_acceleration    0.162104\n       consumo_diff1    0.136196\n      consumo_mean_3    0.074855\n\nRandomForest - Solo_Auto\n------------------------------------------------------------\n             feature  importance\n        consumo_lag7    0.151327\nconsumo_volatility_7    0.123159\n        consumo_lag1    0.115332\nconsumo_acceleration    0.113905\n        consumo_lag8    0.110954\n       consumo_std_7    0.109038\n       consumo_diff1    0.097039\n       consumo_lag19    0.090801\n      consumo_mean_3    0.088444\n\n================================================================================\nGENERANDO VISUALIZACIONES\n================================================================================\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n================================================================================\nCOMPARACI√ìN CON BASELINE\n================================================================================\n\nMejor Baseline: Theta\n  sMAPE: 67.72%\n\nMejor ML: Ridge (Solo_Auto)\n  sMAPE: 46.13%\n\n‚úÖ MEJORA: 31.9% respecto al baseline\n\n================================================================================\nEXPERIMENTACI√ìN DE ML COMPLETADA\n================================================================================\n================================================================================\nGENERANDO VISUALIZACI√ìN DE TOP 4 MODELOS\n================================================================================\n\nTop 4 modelos a visualizar:\n  1. Ridge (Solo_Auto) - sMAPE: 46.13%\n  2. RandomForest (Solo_Auto) - sMAPE: 46.24%\n  3. Ridge (Con_Exogenas) - sMAPE: 46.34%\n  4. LightGBM (Solo_Auto) - sMAPE: 46.53%\n\n√öltimo fold:\n  Train: 487 observaciones hasta 2025-09-10\n  Test: 14 observaciones desde 2025-09-11 hasta 2025-10-08\n\nGenerando predicci√≥n: Ridge (Solo_Auto)...\n\nGenerando predicci√≥n: RandomForest (Solo_Auto)...\n\nGenerando predicci√≥n: Ridge (Con_Exogenas)...\n\nGenerando predicci√≥n: LightGBM (Solo_Auto)...\n\n\n\n\n\n\n\n\n\n\n================================================================================\nAN√ÅLISIS DE ERRORES POR D√çA - TOP 4 MODELOS\n================================================================================\n\nErrores absolutos por d√≠a:\n     Fecha  Real  Ridge (Solo_Auto)_Error_Abs  RandomForest (Solo_Auto)_Error_Abs  Ridge (Con_Exogenas)_Error_Abs  LightGBM (Solo_Auto)_Error_Abs\n2025-09-11     6                     2.371958                            2.164272                        1.837172                        2.107888\n2025-09-12     6                     1.787957                            1.768679                        2.529629                        2.236073\n2025-09-13     3                     0.678084                            1.216242                        1.141840                        0.925746\n2025-09-14     2                     1.578454                            2.535437                        2.359213                        3.666838\n2025-09-15     6                     2.627274                            2.269903                        3.282075                        1.991638\n2025-09-16     1                     2.984872                            3.170091                        3.267369                        3.137177\n2025-09-17     3                     0.721341                            1.225144                        1.647777                        1.418349\n2025-09-30     2                     1.838340                            1.600077                        1.731571                        1.207925\n2025-10-01     4                     0.475908                            0.314133                        0.588942                        0.466834\n2025-10-03     2                     1.781499                            2.042340                        1.706914                        2.178943\n2025-10-05     4                     0.262754                            0.161379                        0.278863                        0.493574\n2025-10-06     5                     0.911993                            0.746581                        1.621441                        0.835828\n2025-10-07     5                     1.118866                            1.274897                        0.696598                        1.379063\n2025-10-08     2                     1.807961                            2.097163                        2.300995                        2.055258\n\n--------------------------------------------------------------------------------\nESTAD√çSTICAS DE ERROR\n--------------------------------------------------------------------------------\n\nRidge (Solo_Auto):\n  MAE: 1.50\n  Error m√°ximo: 2.98\n  Error m√≠nimo: 0.26\n\nRandomForest (Solo_Auto):\n  MAE: 1.61\n  Error m√°ximo: 3.17\n  Error m√≠nimo: 0.16\n\nRidge (Con_Exogenas):\n  MAE: 1.79\n  Error m√°ximo: 3.28\n  Error m√≠nimo: 0.28\n\nLightGBM (Solo_Auto):\n  MAE: 1.72\n  Error m√°ximo: 3.67\n  Error m√≠nimo: 0.47"
  },
  {
    "objectID": "eda_final.html#semanal",
    "href": "eda_final.html#semanal",
    "title": "Informe",
    "section": "3.2 Semanal",
    "text": "3.2 Semanal\n\n3.2.1 Baseline\n\n\n================================================================================\nBASELINE - DATOS SEMANALES\n================================================================================\n\nDatos semanales: 83 per√≠odos\nRango: 2024-03-03 a 2025-10-05\nMedia: 29.06, Desv. Est: 9.50\nCV: 32.68%\n\nConfiguraci√≥n:\n  - Horizonte (H): 10 semanas (~14 d√≠as)\n  - Estacionalidad (m): 4 semanas (~1 mes)\n  - Folds de validaci√≥n: 5\n\nSplits generados:\n  Fold 1: Train 33 semanas | Test 10 semanas\n  Fold 2: Train 43 semanas | Test 10 semanas\n  Fold 3: Train 53 semanas | Test 10 semanas\n  Fold 4: Train 63 semanas | Test 10 semanas\n  Fold 5: Train 73 semanas | Test 10 semanas\n\n================================================================================\nB√öSQUEDA SARIMAX\n================================================================================\nMejor SARIMAX: (1, 1, 1), (0, 0, 1, 4), AIC=188.05\n\n================================================================================\nEVALUANDO MODELOS\n================================================================================\nEvaluando Naive... sMAPE: 43.57%\nEvaluando Seasonal_Naive... sMAPE: 48.15%\nEvaluando MA_4... sMAPE: 39.36%\nEvaluando MA_8... sMAPE: 32.38%\nEvaluando SES... sMAPE: 36.48%\nEvaluando Holt_Winters... sMAPE: 39.08%\nEvaluando Theta... sMAPE: 37.56%\nEvaluando SARIMAX... sMAPE: 34.28%\n\n================================================================================\nRESULTADOS - BASELINE SEMANAL\n================================================================================\n        Modelo  sMAPE_mean  sMAPE_std  MAE_mean  RMSE_mean\n          MA_8   32.382449  11.237996  8.115000   9.901855\n       SARIMAX   34.276356  12.394739  8.587997  10.604816\n           SES   36.478919  15.334106  8.999873  11.009423\n         Theta   37.563641  16.397911  9.160483  11.120807\n  Holt_Winters   39.083603  18.035467  9.385794  11.732814\n          MA_4   39.357768  18.605447  9.640000  11.648762\n         Naive   43.572765  22.457256 10.320000  12.211744\nSeasonal_Naive   48.150544  18.233096 11.800000  14.483509\n\nMEJOR: MA_8 - sMAPE: 32.38% ¬± 11.24%\n\n\n\n\n\n\n\n\n\n\n================================================================================\nBASELINE SEMANAL COMPLETADO\n================================================================================\nBenchmark a superar: sMAPE &lt; 32.38%\n\n\n\n\n3.2.2 Machine learning (Aprendizaje de maquina)\n\n\n================================================================================\nMODELOS ML - DATOS SEMANALES\n================================================================================\n\nDatos semanales con features: 84 per√≠odos\n(Perdidos por NaN: 4)\n\n================================================================================\nFEATURE SELECTION\n================================================================================\n\n1Ô∏è‚É£  SOLO AUTORREGRESIVOS (Top 5):\n       feature       MI\n consumo_std_4 0.110520\n  consumo_lag1 0.060756\n consumo_diff1 0.015770\nconsumo_mean_4 0.000000\n  consumo_lag3 0.000000\n\n2Ô∏è‚É£  CON EX√ìGENAS (Top 9):\n              feature       MI\n     total_cycle_time 0.304627\n        total_payload 0.214201\n      total_empty_kms 0.213834\n            no_cycles 0.179863\n     total_loaded_kms 0.179737\npresion_mantenimiento 0.175857\n  no_correctivos_lag1 0.154165\n          avg_payload 0.138728\n  no_preventivos_lag4 0.128806\n\nDataset autorregresivo: (84, 5)\nDataset con ex√≥genas: (84, 9)\n\nTime Series CV: 5 folds, test_size=2 semanas\n\n================================================================================\nENTRENAMIENTO - DATOS SEMANALES\n================================================================================\n\nüîÑ ESCENARIO 1: Solo autorregresivos\n--------------------------------------------------------------------------------\nEvaluando Ridge... sMAPE: 42.67% ¬± 11.73%\nEvaluando Lasso... sMAPE: 48.02% ¬± 14.41%\nEvaluando RandomForest... sMAPE: 48.55% ¬± 13.81%\nEvaluando LightGBM... sMAPE: 47.50% ¬± 13.44%\nEvaluando XGBoost... sMAPE: 48.08% ¬± 13.86%\n\nüîÑ ESCENARIO 2: Con ex√≥genas\n--------------------------------------------------------------------------------\nEvaluando Ridge... sMAPE: 31.94% ¬± 10.28%\nEvaluando Lasso... sMAPE: 39.82% ¬± 10.46%\nEvaluando RandomForest... sMAPE: 30.14% ¬± 10.69%\nEvaluando LightGBM... sMAPE: 34.72% ¬± 15.82%\nEvaluando XGBoost... sMAPE: 30.97% ¬± 14.33%\n\n================================================================================\nRESULTADOS - ML SEMANAL\n================================================================================\n\nRanking por sMAPE:\n\n      Modelo    Escenario  N_Features  sMAPE_mean  sMAPE_std  MAE_mean  RMSE_mean\nRandomForest Con_Exogenas           9   30.141421  10.692326  6.355032   7.329215\n     XGBoost Con_Exogenas           9   30.969656  14.332593  6.538184   7.462855\n       Ridge Con_Exogenas           9   31.937783  10.277466  7.108505   8.430228\n    LightGBM Con_Exogenas           9   34.720130  15.817097  7.383776   8.349532\n       Lasso Con_Exogenas           9   39.822681  10.460508  8.960246   9.987177\n       Ridge    Solo_Auto           5   42.673334  11.731837  9.661430  11.099435\n    LightGBM    Solo_Auto           5   47.503701  13.438187 11.064338  12.473802\n       Lasso    Solo_Auto           5   48.015599  14.412033 11.181226  12.181206\n     XGBoost    Solo_Auto           5   48.077365  13.859625 11.272826  12.607737\nRandomForest    Solo_Auto           5   48.551512  13.812089 11.394071  12.697973\n\nMEJOR: RandomForest (Con_Exogenas)\n  sMAPE: 30.14% ¬± 10.69%\n  MAE: 6.36\n  Features: 9\n\n--------------------------------------------------------------------------------\nCOMPARACI√ìN POR ESCENARIO\n--------------------------------------------------------------------------------\n\nSolo_Auto:\n  Mejor: Ridge - sMAPE: 42.67%\n\nCon_Exogenas:\n  Mejor: RandomForest - sMAPE: 30.14%\n\n\n\n\n\n\n\n\n\n\n================================================================================\nCOMPARACI√ìN CON BASELINE\n================================================================================\n\nMejor Baseline: MA_8\n  sMAPE: 32.38%\n\nMejor ML: RandomForest (Con_Exogenas)\n  sMAPE: 30.14%\n\n‚úÖ ML MEJORA: 2.24 puntos (6.9%)\n\n================================================================================\nML SEMANAL COMPLETADO\n================================================================================\n================================================================================\nVISUALIZACI√ìN COMPLETA - TODOS LOS FOLDS\n================================================================================\n\nTop 3 modelos:\n  1. RandomForest (Con_Exogenas) - sMAPE: 30.1%\n  2. XGBoost (Con_Exogenas) - sMAPE: 31.0%\n  3. Ridge (Con_Exogenas) - sMAPE: 31.9%\n\nGenerando predicciones en todos los folds...\n  RandomForest (Con_Exogenas)...\n  XGBoost (Con_Exogenas)...\n  Ridge (Con_Exogenas)...\n\n\n\n\n\n\n\n\n\n\n================================================================================\nESTAD√çSTICAS GLOBALES (TODOS LOS FOLDS)\n================================================================================\n\nRandomForest (Con_Exogenas):\n  MAE: 6.36 llantas/semana\n  RMSE: 7.68\n  MAPE: 39.8%\n  Total predicciones: 10 semanas\n  sMAPE (global): 30.1%\nerror forecast: 11.92764515660952\n\nXGBoost (Con_Exogenas):\n  MAE: 6.54 llantas/semana\n  RMSE: 7.95\n  MAPE: 41.7%\n  Total predicciones: 10 semanas\n  sMAPE (global): 31.0%\nerror forecast: 11.848969570426053\n\nRidge (Con_Exogenas):\n  MAE: 7.11 llantas/semana\n  RMSE: 8.95\n  MAPE: 42.6%\n  Total predicciones: 10 semanas\n  sMAPE (global): 31.9%\nerror forecast: 11.21834859026461\n\n================================================================================"
  },
  {
    "objectID": "eda_final.html#quincenal",
    "href": "eda_final.html#quincenal",
    "title": "Informe",
    "section": "3.3 Quincenal",
    "text": "3.3 Quincenal\n\n3.3.1 Baseline\n\n\n================================================================================\nBASELINE - DATOS SEMANALES\n================================================================================\n\nDatos semanales: 39 per√≠odos\nRango: 2024-02-22 a 2025-09-14\nMedia: 62.08, Desv. Est: 16.83\nCV: 27.11%\n\nConfiguraci√≥n:\n  - Horizonte (H): 4 semanas (~14 d√≠as)\n  - Estacionalidad (m): 4 semanas (~1 mes)\n  - Folds de validaci√≥n: 3\n\nSplits generados:\n  Fold 1: Train 27 semanas | Test 4 semanas\n  Fold 2: Train 31 semanas | Test 4 semanas\n  Fold 3: Train 35 semanas | Test 4 semanas\n\n================================================================================\nB√öSQUEDA SARIMAX\n================================================================================\nMejor SARIMAX: (1, 1, 1), (0, 0, 1, 4), AIC=176.13\n\n================================================================================\nEVALUANDO MODELOS\n================================================================================\nEvaluando Naive... sMAPE: 25.26%\nEvaluando Seasonal_Naive... sMAPE: 41.81%\nEvaluando MA_4... sMAPE: 27.20%\nEvaluando MA_8... sMAPE: 25.09%\nEvaluando SES... sMAPE: 24.64%\nEvaluando Holt_Winters... sMAPE: 25.05%\nEvaluando Theta... sMAPE: 25.50%\nEvaluando SARIMAX... sMAPE: 22.45%\n\n================================================================================\nRESULTADOS - BASELINE SEMANAL\n================================================================================\n        Modelo  sMAPE_mean  sMAPE_std  MAE_mean  RMSE_mean\n       SARIMAX   22.451319  11.722553 12.285500  13.343340\n           SES   24.642917  10.526045 13.612496  15.243443\n  Holt_Winters   25.050602  11.423826 13.803248  16.391116\n          MA_8   25.093585  10.533490 13.875000  15.709391\n         Naive   25.261646  11.323094 14.000000  15.653844\n         Theta   25.504570  10.234307 14.046960  15.949813\n          MA_4   27.203472  12.468414 14.916667  16.483613\nSeasonal_Naive   41.813410   5.402722 23.083333  26.272855\n\nMEJOR: SARIMAX - sMAPE: 22.45% ¬± 11.72%\n\n\n\n\n\n\n\n\n\n\n================================================================================\nBASELINE SEMANAL COMPLETADO\n================================================================================\nBenchmark a superar: sMAPE &lt; 22.45%\n\n\n\n\n3.3.2 Machine learning (Aprendizaje de maquina)\n\n\n================================================================================\nMODELOS ML - DATOS SEMANALES\n================================================================================\n\nDatos semanales con features: 36 per√≠odos\n(Perdidos por NaN: 52)\n\n================================================================================\nFEATURE SELECTION\n================================================================================\n\n1Ô∏è‚É£  SOLO AUTORREGRESIVOS (Top 5):\n      feature       MI\nconsumo_std_4 0.211976\n consumo_lag3 0.163258\n consumo_lag2 0.071896\n consumo_lag1 0.000000\nconsumo_diff1 0.000000\n\n2Ô∏è‚É£  CON EX√ìGENAS (Top 6):\n                feature       MI\n          total_payload 0.287566\n       total_loaded_kms 0.258038\n       total_cycle_time 0.240400\n        total_empty_kms 0.229653\nllantas_desechadas_lag1 0.220885\n          consumo_std_4 0.211976\n\nDataset autorregresivo: (36, 5)\nDataset con ex√≥genas: (36, 6)\n\nTime Series CV: 5 folds, test_size=2 semanas\n\n================================================================================\nENTRENAMIENTO - DATOS SEMANALES\n================================================================================\n\nüîÑ ESCENARIO 1: Solo autorregresivos\n--------------------------------------------------------------------------------\nEvaluando Ridge... sMAPE: 28.31% ¬± 18.30%\nEvaluando Lasso... sMAPE: 29.49% ¬± 18.24%\nEvaluando RandomForest... sMAPE: 29.02% ¬± 19.79%\nEvaluando LightGBM... sMAPE: 28.97% ¬± 17.22%\nEvaluando XGBoost... sMAPE: 30.63% ¬± 19.28%\n\nüîÑ ESCENARIO 2: Con ex√≥genas\n--------------------------------------------------------------------------------\nEvaluando Ridge... sMAPE: 12.38% ¬± 3.70%\nEvaluando Lasso... sMAPE: 12.38% ¬± 3.72%\nEvaluando RandomForest... sMAPE: 19.26% ¬± 15.20%\nEvaluando LightGBM... sMAPE: 18.68% ¬± 19.20%\nEvaluando XGBoost... sMAPE: 20.48% ¬± 18.61%\n\n================================================================================\nRESULTADOS - ML SEMANAL\n================================================================================\n\nRanking por sMAPE:\n\n      Modelo    Escenario  N_Features  sMAPE_mean  sMAPE_std  MAE_mean  RMSE_mean\n       Ridge Con_Exogenas           6   12.377722   3.695660  6.643994   8.002529\n       Lasso Con_Exogenas           6   12.379174   3.718091  6.596930   7.910188\n    LightGBM Con_Exogenas           6   18.675006  19.202246  8.892106   9.421178\nRandomForest Con_Exogenas           6   19.258248  15.197565  9.310133  10.109816\n     XGBoost Con_Exogenas           6   20.478698  18.605042 10.112125  11.253438\n       Ridge    Solo_Auto           5   28.307967  18.300989 14.883560  15.214958\n    LightGBM    Solo_Auto           5   28.973109  17.223265 15.467963  15.729950\nRandomForest    Solo_Auto           5   29.015247  19.788476 15.320107  15.462848\n       Lasso    Solo_Auto           5   29.489197  18.236162 15.573613  15.996905\n     XGBoost    Solo_Auto           5   30.630652  19.283265 16.479278  16.807510\n\nMEJOR: Ridge (Con_Exogenas)\n  sMAPE: 12.38% ¬± 3.70%\n  MAE: 6.64\n  Features: 6\n\n--------------------------------------------------------------------------------\nCOMPARACI√ìN POR ESCENARIO\n--------------------------------------------------------------------------------\n\nSolo_Auto:\n  Mejor: Ridge - sMAPE: 28.31%\n\nCon_Exogenas:\n  Mejor: Ridge - sMAPE: 12.38%\n\n\n\n\n\n\n\n\n\n\n================================================================================\nCOMPARACI√ìN CON BASELINE\n================================================================================\n\nMejor Baseline: SARIMAX\n  sMAPE: 22.45%\n\nMejor ML: Ridge (Con_Exogenas)\n  sMAPE: 12.38%\n\n‚úÖ ML MEJORA: 10.07 puntos (44.9%)\n\n================================================================================\nML SEMANAL COMPLETADO\n================================================================================\n================================================================================\nVISUALIZACI√ìN COMPLETA - TODOS LOS FOLDS\n================================================================================\n\nTop 3 modelos:\n  1. Ridge (Con_Exogenas) - sMAPE: 12.4%\n  2. Lasso (Con_Exogenas) - sMAPE: 12.4%\n  3. LightGBM (Con_Exogenas) - sMAPE: 18.7%\n\nGenerando predicciones en todos los folds...\n  Ridge (Con_Exogenas)...\n  Lasso (Con_Exogenas)...\n  LightGBM (Con_Exogenas)...\n\n\n\n\n\n\n\n\n\n\n================================================================================\nESTAD√çSTICAS GLOBALES (TODOS LOS FOLDS)\n================================================================================\n\nRidge (Con_Exogenas):\n  MAE: 6.64 llantas/semana\n  RMSE: 8.35\n  MAPE: 13.5%\n  Total predicciones: 10 semanas\n  sMAPE (global): 12.4%\nerror forecast: 6.086849396001813\n\nLasso (Con_Exogenas):\n  MAE: 6.60 llantas/semana\n  RMSE: 8.26\n  MAPE: 13.5%\n  Total predicciones: 10 semanas\n  sMAPE (global): 12.4%\nerror forecast: 6.367319520653622\n\nLightGBM (Con_Exogenas):\n  MAE: 8.89 llantas/semana\n  RMSE: 11.66\n  MAPE: 23.4%\n  Total predicciones: 10 semanas\n  sMAPE (global): 18.7%\nerror forecast: 15.305834853130582\n\n================================================================================"
  },
  {
    "objectID": "eda_final.html#diferente-granularidad-temporal",
    "href": "eda_final.html#diferente-granularidad-temporal",
    "title": "Informe",
    "section": "4.1 Diferente granularidad temporal",
    "text": "4.1 Diferente granularidad temporal\n\n\n================================================================================\nAN√ÅLISIS EXPLORATORIO - DATOS AGREGADOS\n================================================================================\n\nDatos originales (diarios): 481 registros\nPer√≠odo: 2024-02-07 a 2025-10-09\n\nAgregando datos...\n\nDatos semanales: 70 per√≠odos\nDatos quincenales: 34 per√≠odos\nDatos mensuales: 18 per√≠odos\n\n================================================================================\nESTAD√çSTICAS DESCRIPTIVAS POR FRECUENCIA Y TARGET\n================================================================================\n         Target Frecuencia  N_per√≠odos     Media  Mediana  Desv_Est       CV_%  Min   Max    Q1     Q3\n  llantas_total     Diario         481  3.817048      4.0  2.096575  54.926603  1.0  12.0  2.00   5.00\n  llantas_total    Semanal          70 23.728571     23.0  6.797104  28.645230 10.0  42.0 18.00  28.00\n  llantas_total  Quincenal          34 49.294118     47.5 12.566259  25.492412 25.0  73.0 39.25  56.75\n  llantas_total    Mensual          18 97.277778     99.5 19.411354  19.954561 57.0 130.0 87.00 106.00\n llantas_CAT789     Diario         481  0.893971      0.0  1.289050 144.193783  0.0   6.0  0.00   2.00\n llantas_CAT789    Semanal          70  5.700000      6.0  3.218380  56.462810  0.0  12.0  4.00   8.00\n llantas_CAT789  Quincenal          34 11.852941     11.5  6.105796  51.512922  0.0  24.0  6.50  16.00\n llantas_CAT789    Mensual          18 22.666667     24.0 10.588562  46.714244  0.0  39.0 18.25  28.00\n llantas_CAT793     Diario         481  2.353430      2.0  1.877452  79.775133  0.0   9.0  1.00   4.00\n llantas_CAT793    Semanal          70 14.614286     14.0  4.703661  32.185366  4.0  26.0 11.25  17.00\n llantas_CAT793  Quincenal          34 30.235294     30.5  8.079375  26.721667 16.0  46.0 25.00  34.00\n llantas_CAT793    Mensual          18 60.055556     60.0  9.643481  16.057601 38.0  78.0 56.00  65.00\nllantas_HIT4000     Diario         481  0.133056      0.0  0.542917 408.036275  0.0   5.0  0.00   0.00\nllantas_HIT4000    Semanal          70  0.771429      0.0  1.616683 209.570046  0.0  10.0  0.00   1.00\nllantas_HIT4000  Quincenal          34  1.588235      1.0  2.119429 133.445520  0.0  10.0  0.00   2.00\nllantas_HIT4000    Mensual          18  3.444444      2.0  3.221659  93.532028  0.0  12.0  2.00   4.00\nllantas_KOM930E     Diario         481  0.436590      0.0  0.844487 193.427744  0.0   4.0  0.00   0.00\nllantas_KOM930E    Semanal          70  2.642857      2.0  2.346973  88.804378  0.0  10.0  1.00   4.00\nllantas_KOM930E  Quincenal          34  5.617647      5.5  3.375623  60.089624  0.0  12.0  4.00   8.00\nllantas_KOM930E    Mensual          18 11.111111     13.5  4.663865  41.974782  2.0  16.0  7.25  14.00\n\nDistribuciones para llantas_total\n\n\n\n\n\n\n\n\n\n\nDistribuciones para llantas_CAT789\n\n\n\n\n\n\n\n\n\n\nDistribuciones para llantas_CAT793\n\n\n\n\n\n\n\n\n\n\nDistribuciones para llantas_HIT4000\n\n\n\n\n\n\n\n\n\n\nDistribuciones para llantas_KOM930E\n\n\n\n\n\n\n\n\n\n\nSerie temporal para llantas_total\n\n\n\n\n\n\n\n\n\n\nSerie temporal para llantas_CAT789\n\n\n\n\n\n\n\n\n\n\nSerie temporal para llantas_CAT793\n\n\n\n\n\n\n\n\n\n\nSerie temporal para llantas_HIT4000\n\n\n\n\n\n\n\n\n\n\nSerie temporal para llantas_KOM930E\n\n\n\n\n\n\n\n\n\n\n=== ADF Test para llantas_total ===\nDiario: p-value=0.0000 -&gt; Estacionaria\nSemanal: p-value=0.0000 -&gt; Estacionaria\nQuincenal: p-value=0.7099 -&gt; No estacionaria\nMensual: p-value=0.1080 -&gt; No estacionaria\n\n=== ADF Test para llantas_CAT789 ===\nDiario: p-value=0.0000 -&gt; Estacionaria\nSemanal: p-value=0.0000 -&gt; Estacionaria\nQuincenal: p-value=0.5811 -&gt; No estacionaria\nMensual: p-value=0.9928 -&gt; No estacionaria\n\n=== ADF Test para llantas_CAT793 ===\nDiario: p-value=0.0000 -&gt; Estacionaria\nSemanal: p-value=0.0000 -&gt; Estacionaria\nQuincenal: p-value=0.0000 -&gt; Estacionaria\nMensual: p-value=0.9638 -&gt; No estacionaria\n\n=== ADF Test para llantas_HIT4000 ===\nDiario: p-value=0.0000 -&gt; Estacionaria\nSemanal: p-value=0.0000 -&gt; Estacionaria\nQuincenal: p-value=0.0000 -&gt; Estacionaria\nMensual: p-value=0.1593 -&gt; No estacionaria\n\n=== ADF Test para llantas_KOM930E ===\nDiario: p-value=0.0000 -&gt; Estacionaria\nSemanal: p-value=0.0000 -&gt; Estacionaria\nQuincenal: p-value=0.0000 -&gt; Estacionaria\nMensual: p-value=0.0013 -&gt; Estacionaria\n\n=== ACF/PACF para llantas_total ===\n\n\n\n\n\n\n\n\n\n\n=== ACF/PACF para llantas_CAT789 ===\n\n\n\n\n\n\n\n\n\n\n=== ACF/PACF para llantas_CAT793 ===\n\n\n\n\n\n\n\n\n\n\n=== ACF/PACF para llantas_HIT4000 ===\n\n\n\n\n\n\n\n\n\n\n=== ACF/PACF para llantas_KOM930E ===\n\n\n\n\n\n\n\n\n\n\n4.1.1 Modelamiento Semanal\n\n4.1.1.0.1 Parametrico\n\n\n================================================================================\nBASELINE - DATOS SEMANALES\n================================================================================\n\nDatos semanales: 70 per√≠odos\nRango: 2024-03-03 a 2025-09-21\nMedia: 23.73, Desv. Est: 6.80\nCV: 28.65%\n\nConfiguraci√≥n:\n  - Horizonte (H): 10 semanas (~14 d√≠as)\n  - Estacionalidad (m): 4 semanas (~1 mes)\n  - Folds de validaci√≥n: 5\n\nSplits generados:\n  Fold 1: Train 20 semanas | Test 10 semanas\n  Fold 2: Train 30 semanas | Test 10 semanas\n  Fold 3: Train 40 semanas | Test 10 semanas\n  Fold 4: Train 50 semanas | Test 10 semanas\n  Fold 5: Train 60 semanas | Test 10 semanas\n\n================================================================================\nB√öSQUEDA SARIMAX\n================================================================================\nMejor SARIMAX: (1, 1, 1), (0, 0, 1, 4), AIC=84.89\n\n================================================================================\nEVALUANDO MODELOS\n================================================================================\nEvaluando Naive... sMAPE: 34.72%\nEvaluando Seasonal_Naive... sMAPE: 37.86%\nEvaluando MA_4... sMAPE: 31.38%\nEvaluando MA_8... sMAPE: 26.54%\nEvaluando SES... sMAPE: 28.09%\nEvaluando Holt_Winters... sMAPE: 30.75%\nEvaluando Theta... sMAPE: 27.27%\nEvaluando SARIMAX... sMAPE: 40.53%\n\n================================================================================\nRESULTADOS - BASELINE SEMANAL\n================================================================================\n        Modelo  sMAPE_mean  sMAPE_std  MAE_mean  RMSE_mean\n          MA_8   26.537586   4.279008  6.120000   7.421222\n         Theta   27.269421   4.092889  6.293256   7.316142\n           SES   28.090522   2.996790  6.489411   7.480554\n  Holt_Winters   30.747292   8.931601  7.036422   8.426383\n          MA_4   31.376668   4.265648  7.280000   8.405114\n         Naive   34.722231  15.149885  8.660000  10.164884\nSeasonal_Naive   37.862500   9.196434  8.920000  10.374111\n       SARIMAX   40.525184  30.192289  8.345169   9.493738\n\nMEJOR: MA_8 - sMAPE: 26.54% ¬± 4.28%\n\n\n\n\n\n\n\n\n\n\n================================================================================\nBASELINE SEMANAL COMPLETADO\n================================================================================\nBenchmark a superar: sMAPE &lt; 26.54%\n\n\n\n\n4.1.1.1 No parametrico (Machine learning)\n\n\n================================================================================\nMODELOS ML - DATOS SEMANALES\n================================================================================\n\nDatos semanales con features: 66 per√≠odos\n(Perdidos por NaN: 4)\n\n================================================================================\nFEATURE SELECTION\n================================================================================\n\n1Ô∏è‚É£  SOLO AUTORREGRESIVOS (Top 5):\n       feature       MI\n consumo_diff1 0.076356\n  consumo_lag3 0.071220\n consumo_std_4 0.062036\n  consumo_lag2 0.000000\nconsumo_mean_4 0.000000\n\n2Ô∏è‚É£  CON EX√ìGENAS (Top 7):\n             feature       MI\n    total_loaded_kms 0.155269\n no_correctivos_lag1 0.143427\n       total_payload 0.120690\n           Rain_lag3 0.118133\n           no_cycles 0.098809\nno_inspe_prio_2_lag4 0.082091\n       consumo_diff1 0.076356\n\nDataset autorregresivo: (66, 5)\nDataset con ex√≥genas: (66, 7)\n\nTime Series CV: 5 folds, test_size=2 semanas\n\n================================================================================\nENTRENAMIENTO - DATOS SEMANALES\n================================================================================\n\nüîÑ ESCENARIO 1: Solo autorregresivos\n--------------------------------------------------------------------------------\nEvaluando Ridge... sMAPE: 34.87% ¬± 19.34%\nEvaluando Lasso... sMAPE: 33.55% ¬± 16.59%\nEvaluando RandomForest... sMAPE: 34.90% ¬± 17.48%\nEvaluando LightGBM... sMAPE: 35.07% ¬± 16.73%\nEvaluando XGBoost... sMAPE: 37.06% ¬± 17.99%\n\nüîÑ ESCENARIO 2: Con ex√≥genas\n--------------------------------------------------------------------------------\nEvaluando Ridge... sMAPE: 34.69% ¬± 17.71%\nEvaluando Lasso... sMAPE: 32.67% ¬± 16.58%\nEvaluando RandomForest... sMAPE: 29.44% ¬± 11.99%\nEvaluando LightGBM... sMAPE: 27.69% ¬± 12.21%\nEvaluando XGBoost... sMAPE: 28.02% ¬± 11.01%\n\n================================================================================\nRESULTADOS - ML SEMANAL\n================================================================================\n\nRanking por sMAPE:\n\n      Modelo    Escenario  N_Features  sMAPE_mean  sMAPE_std  MAE_mean  RMSE_mean\n    LightGBM Con_Exogenas           7   27.694209  12.205604  6.257043   6.586857\n     XGBoost Con_Exogenas           7   28.015004  11.009565  6.248014   6.567505\nRandomForest Con_Exogenas           7   29.441100  11.986676  6.680166   7.005563\n       Lasso Con_Exogenas           7   32.672058  16.584172  7.548490   8.338397\n       Lasso    Solo_Auto           5   33.552118  16.592380  7.762813   8.641138\n       Ridge Con_Exogenas           7   34.687060  17.711479  6.793327   7.000042\n       Ridge    Solo_Auto           5   34.867622  19.340841  8.213938   8.840472\nRandomForest    Solo_Auto           5   34.901174  17.483422  8.125409   8.805048\n    LightGBM    Solo_Auto           5   35.071424  16.726081  8.084391   8.444199\n     XGBoost    Solo_Auto           5   37.058213  17.985877  8.629880   9.081610\n\nMEJOR: LightGBM (Con_Exogenas)\n  sMAPE: 27.69% ¬± 12.21%\n  MAE: 6.26\n  Features: 7\n\n--------------------------------------------------------------------------------\nCOMPARACI√ìN POR ESCENARIO\n--------------------------------------------------------------------------------\n\nSolo_Auto:\n  Mejor: Lasso - sMAPE: 33.55%\n\nCon_Exogenas:\n  Mejor: LightGBM - sMAPE: 27.69%\n\n\n\n\n\n\n\n\n\n\n================================================================================\nCOMPARACI√ìN CON BASELINE\n================================================================================\n\nMejor Baseline: MA_8\n  sMAPE: 26.54%\n\nMejor ML: LightGBM (Con_Exogenas)\n  sMAPE: 27.69%\n\n‚ùå ML NO supera baseline: -1.16 puntos\n\nResultados guardados en 'ml_semanal_resultados.csv'\n\n================================================================================\nML SEMANAL COMPLETADO\n================================================================================\n\n\n\n\n\n4.1.2 TOP-DOWN Forecasting\n\n\n================================================================================\nTOP-DOWN FORECASTING + MinT RECONCILIATION\n================================================================================\n\n================================================================================\nPREPARACI√ìN DE DATOS\n================================================================================\nCategor√≠as: ['llantas_CAT789', 'llantas_CAT793_agrupado', 'llantas_KOM930E']\nProporciones: ['prop_llantas_CAT789', 'prop_llantas_CAT793_agrupado', 'prop_llantas_KOM930E']\n\n================================================================================\nFEATURE ENGINEERING\n================================================================================\nDatos despu√©s de feature engineering: 66 per√≠odos\nColumnas totales: 59\n\n================================================================================\nFEATURE SELECTION\n================================================================================\n\nüìä FEATURES PARA EL TOTAL (Top 5):\n         feature       MI\n      semana_num 0.229360\n horas_down_lag1 0.160623\ntotal_loaded_kms 0.148039\n       Rain_lag3 0.125947\n   total_payload 0.119184\n\nüìä FEATURES CANDIDATAS PARA PROPORCIONES: 19\n\nprop_llantas_CAT789 (Top 5):\n                feature       MI\n             semana_num 0.186614\n         no_cycles_norm 0.076033\n    mantenimiento_total 0.053960\nprop_llantas_CAT789_ma4 0.048660\n             semana_sin 0.047233\n\nprop_llantas_CAT793_agrupado (Top 5):\n                  feature       MI\n  prop_llantas_CAT789_ma4 0.253433\n      mantenimiento_total 0.151510\n         intensidad_carga 0.108323\nprop_llantas_KOM930E_lag1 0.079732\n                 km_total 0.076834\n\nprop_llantas_KOM930E (Top 5):\n                  feature       MI\n               semana_num 0.152596\n               semana_sin 0.147831\n         avg_payload_norm 0.129295\n      mantenimiento_total 0.113248\nprop_llantas_KOM930E_lag2 0.073947\n\nüìä FEATURES √öNICOS PARA PROPORCIONES: 10\n\n================================================================================\nEVALUACI√ìN CON TIME SERIES CV\n================================================================================\nTime Series CV: 5 folds, horizonte=2 semanas\n\nEvaluando combinaciones de modelos...\n\nüîÑ Evaluando: Total=Ridge, Props=Ridge\n  MAE Total (TopDown): 6.89\n  MAE Total (MinT): 6.89\n  Mejora: 0.0%\n\nüîÑ Evaluando: Total=RandomForest, Props=Ridge\n  MAE Total (TopDown): 6.73\n  MAE Total (MinT): 6.73\n  Mejora: -0.0%\n\nüîÑ Evaluando: Total=LightGBM, Props=Ridge\n  MAE Total (TopDown): 6.80\n  MAE Total (MinT): 6.80\n  Mejora: 0.0%\n\nüîÑ Evaluando: Total=RandomForest, Props=RandomForest\n  MAE Total (TopDown): 6.73\n  MAE Total (MinT): 6.73\n  Mejora: -0.0%\n\nüîÑ Evaluando: Total=LightGBM, Props=LightGBM\n  MAE Total (TopDown): 6.80\n  MAE Total (MinT): 6.80\n  Mejora: 0.0%\n\n\n================================================================================\nRESULTADOS - TOP-DOWN + MinT\n================================================================================\n\nüìä RANKING POR MAE TOTAL (con MinT):\n\nModelo_Total Modelo_Props  MAE_Total_MinT  MAE_Cats_MinT  Mejora_Total_MinT_%  Coherence_MinT\nRandomForest        Ridge        6.732209       2.729304        -3.957891e-14    2.131628e-15\nRandomForest RandomForest        6.732209       2.643768        -6.596486e-14    3.197442e-15\n    LightGBM     LightGBM        6.799266       2.611909         3.918857e-14    1.421085e-15\n    LightGBM        Ridge        6.799266       2.767462         0.000000e+00    2.842171e-15\n       Ridge        Ridge        6.888707       2.714158         2.578651e-14    2.042810e-15\n\nüèÜ MEJOR CONFIGURACI√ìN:\n   Total: RandomForest\n   Proporciones: Ridge\n   MAE Total (MinT): 6.73\n   MAE Categor√≠as (MinT): 2.73\n   Mejora MinT vs TopDown: -0.0%\n   Error Coherencia: 0.000000\n\n================================================================================\nCOMPARACI√ìN: TopDown Simple vs MinT Reconciliation\n================================================================================\n\n              Configuraci√≥n  MAE_TopDown  MAE_MinT      Mejora_%\n       RandomForest + Ridge     6.732209  6.732209 -3.957891e-14\nRandomForest + RandomForest     6.732209  6.732209 -6.596486e-14\n        LightGBM + LightGBM     6.799266  6.799266  3.918857e-14\n           LightGBM + Ridge     6.799266  6.799266  0.000000e+00\n              Ridge + Ridge     6.888707  6.888707  2.578651e-14\n\nüìà Mejora promedio de MinT: -0.0%\n‚ùå MinT no mejora - Usar TopDown simple\n\n================================================================================\nAN√ÅLISIS DE SENSIBILIDAD: M√©todos MinT\n================================================================================\n\nComparando m√©todos MinT con: RandomForest + Ridge\n\nüîÑ Evaluando MinT method: ols\n  MAE Total: 6.73\n  Mejora: 0.0%\n\nüîÑ Evaluando MinT method: wls_struct\n  MAE Total: 6.73\n  Mejora: -0.0%\n\nüîÑ Evaluando MinT method: mint_shrink\n  MAE Total: 6.73\n  Mejora: 0.0%\n\nüìä COMPARACI√ìN DE M√âTODOS MinT:\nMinT_Method  MAE_Total_MinT  MAE_Cats_MinT  Mejora_Total_MinT_%  Coherence_MinT\n        ols        6.732209       2.729304         3.957891e-14    1.776357e-15\n wls_struct        6.732209       2.729304        -3.957891e-14    2.486900e-15\nmint_shrink        6.732209       2.729304         3.957891e-14    1.776357e-15\n\nüèÜ MEJOR M√âTODO MinT: ols\n   MAE Total: 6.73\n   Mejora vs TopDown: 0.0%\n\n================================================================================\nGENERANDO VISUALIZACIONES\n================================================================================\n\n================================================================================\nGUARDANDO RESULTADOS\n================================================================================\n‚úì Guardado: topdown_mint_resumen.csv\n\n================================================================================\nRECOMENDACIONES FINALES\n================================================================================\n\nüéØ CONFIGURACI√ìN √ìPTIMA IDENTIFICADA:\n\n1. MODELO PARA EL TOTAL:\n   ‚Üí RandomForest\n   ‚Üí MAE: 6.73 llantas\n   ‚Üí Features usadas: 5\n\n2. MODELO PARA PROPORCIONES:\n   ‚Üí Ridge\n   ‚Üí MAE promedio categor√≠as: 2.73 llantas\n   ‚Üí Features usadas: 10\n\n3. M√âTODO DE RECONCILIACI√ìN:\n   ‚Üí ols\n   ‚Üí Mejora vs TopDown simple: 0.0%\n   ‚Üí Error de coherencia: 0.000000 (‚âà0 = perfecto)\n\nüìä COMPARACI√ìN DE ENFOQUES:\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ      M√âTRICA         ‚îÇ   TopDown   ‚îÇ     MinT     ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ MAE Total            ‚îÇ   6.73     ‚îÇ    6.73      ‚îÇ\n‚îÇ MAE Categor√≠as       ‚îÇ   2.73     ‚îÇ    2.73      ‚îÇ\n‚îÇ Error Coherencia     ‚îÇ   0.0000   ‚îÇ    0.000000  ‚îÇ\n‚îÇ Mejora %             ‚îÇ      -      ‚îÇ    -0.0%     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n‚úÖ VENTAJAS DEL SISTEMA TOP-DOWN + MinT:\n\n‚Ä¢ COHERENCIA PERFECTA: Las predicciones por categor√≠a siempre suman el total\n  (Error MinT ‚âà 0 vs TopDown = 0.0000)\n\n‚Ä¢ MEJOR PRECISI√ìN: MinT mejora -0.0% en promedio\n  (Usa correlaciones entre errores de diferentes categor√≠as)\n\n‚Ä¢ ROBUSTEZ: Con 66 observaciones, modelos regularizados evitan overfitting\n\n‚Ä¢ INTERPRETABILIDAD:\n  - Total: captura demanda agregada\n  - Proporciones: capturan cambios en composici√≥n de flota\n\nüìà PR√ìXIMOS PASOS:\n\n1. OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS:\n   ‚úì GridSearchCV para el modelo del total\n   ‚úì Ajustar shrinkage_factor en MinT (probar 0.2, 0.3, 0.4)\n\n2. FEATURES ADICIONALES:\n   ‚úì Informaci√≥n de inventario de llantas\n   ‚úì Variables clim√°ticas m√°s granulares\n   ‚úì D√≠as festivos/paradas programadas\n\n3. VALIDACI√ìN TEMPORAL:\n   ‚úì Walk-forward validation con ventana m√≥vil\n   ‚úì Evaluar estabilidad en el tiempo\n\n4. PRODUCCI√ìN:\n   ‚úì Entrenar modelo final con TODOS los datos\n   ‚úì Generar forecasts para pr√≥ximas 2 semanas\n   ‚úì Monitorear errores en tiempo real\n\n‚ö†Ô∏è  CONSIDERACIONES IMPORTANTES:\n\n‚Ä¢ Dataset: 66 semanas ‚Üí mantener modelos simples (Ridge/Lasso preferibles)\n‚Ä¢ Si en futuro tienes &gt;150 semanas, considera modelos m√°s complejos (LightGBM/XGBoost)\n‚Ä¢ Reevaluar modelo cada 3-6 meses con datos nuevos\n‚Ä¢ MinT 'ols' es ideal para datasets peque√±os\n\n\n================================================================================\nFUNCI√ìN DE FORECAST PARA PRODUCCI√ìN\n================================================================================\n\nüîß Entrenando modelo FINAL para producci√≥n...\n   Total: RandomForest\n   Proporciones: Ridge\n   Reconciliaci√≥n: ols\n\n‚úÖ Modelo entrenado con √©xito\n   Observaciones usadas: 66\n   Features Total: 5\n   Features Props: 10\n\n================================================================================\n‚úÖ IMPLEMENTACI√ìN COMPLETA FINALIZADA\n================================================================================\n\nüéØ MODELO FINAL ENTRENADO Y LISTO:\n   ‚Ä¢ Total: RandomForest\n   ‚Ä¢ Proporciones: Ridge\n   ‚Ä¢ Reconciliaci√≥n: ols\n   ‚Ä¢ MAE esperado: 6.73 llantas\n   ‚Ä¢ Mejora vs TopDown: -0.0%\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluando: Total=Ridge, Props=Ridge\nEvaluando: Total=RandomForest, Props=Ridge\nEvaluando: Total=LightGBM, Props=Ridge\nEvaluando: Total=RandomForest, Props=RandomForest\nEvaluando: Total=LightGBM, Props=LightGBM\nVisualizaci√≥n guardada: cv_predictions_complete.png\n\n\n\n\n\n\n\n\n\n\n\n4.1.3 Modelamiento Quincenal\n\n\n================================================================================\nTOP-DOWN FORECASTING + MinT RECONCILIATION\n================================================================================\n\n================================================================================\nPREPARACI√ìN DE DATOS\n================================================================================\nCategor√≠as: ['llantas_CAT789', 'llantas_CAT793_agrupado', 'llantas_KOM930E']\nProporciones: ['prop_llantas_CAT789', 'prop_llantas_CAT793_agrupado', 'prop_llantas_KOM930E']\n\n================================================================================\nFEATURE ENGINEERING\n================================================================================\nDatos despu√©s de feature engineering: 30 per√≠odos\nColumnas totales: 59\n\n================================================================================\nFEATURE SELECTION\n================================================================================\n\nüìä FEATURES PARA EL TOTAL (Top 5):\n             feature       MI\nno_inspe_prio_2_lag4 0.256344\n    total_cycle_time 0.253675\n      no_cycles_norm 0.239830\n           no_cycles 0.239830\n    total_loaded_kms 0.228195\n\nüìä FEATURES CANDIDATAS PARA PROPORCIONES: 19\n\nprop_llantas_CAT789 (Top 5):\n                feature       MI\n       avg_payload_norm 0.275965\n         intensidad_uso 0.191655\n       intensidad_carga 0.184133\nllantas_desechadas_lag1 0.116698\nprop_llantas_CAT789_ma4 0.112890\n\nprop_llantas_CAT793_agrupado (Top 5):\n                 feature       MI\n     mantenimiento_total 0.154442\n          intensidad_uso 0.145045\nprop_llantas_CAT789_lag2 0.106122\nprop_llantas_CAT789_lag1 0.087886\n prop_llantas_CAT789_ma4 0.081288\n\nprop_llantas_KOM930E (Top 5):\n                          feature       MI\n          llantas_desechadas_lag1 0.151691\n prop_llantas_CAT793_agrupado_ma4 0.090838\n                 avg_payload_norm 0.082420\n        prop_llantas_KOM930E_lag1 0.067777\nprop_llantas_CAT793_agrupado_lag1 0.049173\n\nüìä FEATURES √öNICOS PARA PROPORCIONES: 11\n\n================================================================================\nEVALUACI√ìN CON TIME SERIES CV\n================================================================================\nTime Series CV: 5 folds, horizonte=2 semanas\n\nEvaluando combinaciones de modelos...\n\nüîÑ Evaluando: Total=Ridge, Props=Ridge\n  MAE Total (TopDown): 4.75\n  MAE Total (MinT): 4.75\n  Mejora: 0.0%\n\nüîÑ Evaluando: Total=RandomForest, Props=Ridge\n  MAE Total (TopDown): 8.66\n  MAE Total (MinT): 8.66\n  Mejora: 0.0%\n\nüîÑ Evaluando: Total=LightGBM, Props=Ridge\n  MAE Total (TopDown): 9.66\n  MAE Total (MinT): 9.66\n  Mejora: -0.0%\n\nüîÑ Evaluando: Total=RandomForest, Props=RandomForest\n  MAE Total (TopDown): 8.66\n  MAE Total (MinT): 8.66\n  Mejora: -0.0%\n\nüîÑ Evaluando: Total=LightGBM, Props=LightGBM\n  MAE Total (TopDown): 9.66\n  MAE Total (MinT): 9.66\n  Mejora: -0.0%\n\n\n================================================================================\nRESULTADOS - TOP-DOWN + MinT\n================================================================================\n\nüìä RANKING POR MAE TOTAL (con MinT):\n\nModelo_Total Modelo_Props  MAE_Total_MinT  MAE_Cats_MinT  Mejora_Total_MinT_%  Coherence_MinT\n       Ridge        Ridge        4.749102       3.268317         0.000000e+00    4.263256e-15\nRandomForest        Ridge        8.655476       4.122979         4.104585e-14    3.552714e-15\nRandomForest RandomForest        8.655476       4.125059        -1.026146e-13    5.684342e-15\n    LightGBM        Ridge        9.663567       4.519185        -1.838200e-14    9.947598e-15\n    LightGBM     LightGBM        9.663567       4.522287        -3.676400e-14    6.394885e-15\n\nüèÜ MEJOR CONFIGURACI√ìN:\n   Total: Ridge\n   Proporciones: Ridge\n   MAE Total (MinT): 4.75\n   MAE Categor√≠as (MinT): 3.27\n   Mejora MinT vs TopDown: 0.0%\n   Error Coherencia: 0.000000\n\n================================================================================\nCOMPARACI√ìN: TopDown Simple vs MinT Reconciliation\n================================================================================\n\n              Configuraci√≥n  MAE_TopDown  MAE_MinT      Mejora_%\n              Ridge + Ridge     4.749102  4.749102  0.000000e+00\n       RandomForest + Ridge     8.655476  8.655476  4.104585e-14\nRandomForest + RandomForest     8.655476  8.655476 -1.026146e-13\n           LightGBM + Ridge     9.663567  9.663567 -1.838200e-14\n        LightGBM + LightGBM     9.663567  9.663567 -3.676400e-14\n\nüìà Mejora promedio de MinT: -0.0%\n‚ùå MinT no mejora - Usar TopDown simple\n\n================================================================================\nAN√ÅLISIS DE SENSIBILIDAD: M√©todos MinT\n================================================================================\n\nComparando m√©todos MinT con: Ridge + Ridge\n\nüîÑ Evaluando MinT method: ols\n  MAE Total: 4.75\n  Mejora: 0.0%\n\nüîÑ Evaluando MinT method: wls_struct\n  MAE Total: 4.75\n  Mejora: -0.0%\n\nüîÑ Evaluando MinT method: mint_shrink\n  MAE Total: 4.75\n  Mejora: 0.0%\n\nüìä COMPARACI√ìN DE M√âTODOS MinT:\nMinT_Method  MAE_Total_MinT  MAE_Cats_MinT  Mejora_Total_MinT_%  Coherence_MinT\n        ols        4.749102       3.268317         0.000000e+00    2.486900e-15\n wls_struct        4.749102       3.268317        -1.870203e-14    7.105427e-15\nmint_shrink        4.749102       3.268317         0.000000e+00    4.263256e-15\n\nüèÜ MEJOR M√âTODO MinT: ols\n   MAE Total: 4.75\n   Mejora vs TopDown: 0.0%\n\n================================================================================\nGENERANDO VISUALIZACIONES\n================================================================================\n\n================================================================================\nGUARDANDO RESULTADOS\n================================================================================\n\n================================================================================\nRECOMENDACIONES FINALES\n================================================================================\n\nüéØ CONFIGURACI√ìN √ìPTIMA IDENTIFICADA:\n\n1. MODELO PARA EL TOTAL:\n   ‚Üí Ridge\n   ‚Üí MAE: 4.75 llantas\n   ‚Üí Features usadas: 5\n\n2. MODELO PARA PROPORCIONES:\n   ‚Üí Ridge\n   ‚Üí MAE promedio categor√≠as: 3.27 llantas\n   ‚Üí Features usadas: 11\n\n3. M√âTODO DE RECONCILIACI√ìN:\n   ‚Üí ols\n   ‚Üí Mejora vs TopDown simple: 0.0%\n   ‚Üí Error de coherencia: 0.000000 (‚âà0 = perfecto)\n\nüìä COMPARACI√ìN DE ENFOQUES:\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ      M√âTRICA         ‚îÇ   TopDown   ‚îÇ     MinT     ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ MAE Total            ‚îÇ   4.75     ‚îÇ    4.75      ‚îÇ\n‚îÇ MAE Categor√≠as       ‚îÇ   3.27     ‚îÇ    3.27      ‚îÇ\n‚îÇ Error Coherencia     ‚îÇ   0.0000   ‚îÇ    0.000000  ‚îÇ\n‚îÇ Mejora %             ‚îÇ      -      ‚îÇ    +0.0%     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n‚úÖ VENTAJAS DEL SISTEMA TOP-DOWN + MinT:\n\n‚Ä¢ COHERENCIA PERFECTA: Las predicciones por categor√≠a siempre suman el total\n  (Error MinT ‚âà 0 vs TopDown = 0.0000)\n\n‚Ä¢ MEJOR PRECISI√ìN: MinT mejora -0.0% en promedio\n  (Usa correlaciones entre errores de diferentes categor√≠as)\n\n‚Ä¢ ROBUSTEZ: Con 30 observaciones, modelos regularizados evitan overfitting\n\n‚Ä¢ INTERPRETABILIDAD:\n  - Total: captura demanda agregada\n  - Proporciones: capturan cambios en composici√≥n de flota\n\nüìà PR√ìXIMOS PASOS:\n\n1. OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS:\n   ‚úì GridSearchCV para el modelo del total\n   ‚úì Ajustar shrinkage_factor en MinT (probar 0.2, 0.3, 0.4)\n\n2. FEATURES ADICIONALES:\n   ‚úì Informaci√≥n de inventario de llantas\n   ‚úì Variables clim√°ticas m√°s granulares\n   ‚úì D√≠as festivos/paradas programadas\n\n3. VALIDACI√ìN TEMPORAL:\n   ‚úì Walk-forward validation con ventana m√≥vil\n   ‚úì Evaluar estabilidad en el tiempo\n\n4. PRODUCCI√ìN:\n   ‚úì Entrenar modelo final con TODOS los datos\n   ‚úì Generar forecasts para pr√≥ximas 2 semanas\n   ‚úì Monitorear errores en tiempo real\n\n‚ö†Ô∏è  CONSIDERACIONES IMPORTANTES:\n\n‚Ä¢ Dataset: 30 semanas ‚Üí mantener modelos simples (Ridge/Lasso preferibles)\n‚Ä¢ Si en futuro tienes &gt;150 semanas, considera modelos m√°s complejos (LightGBM/XGBoost)\n‚Ä¢ Reevaluar modelo cada 3-6 meses con datos nuevos\n‚Ä¢ MinT 'ols' es ideal para datasets peque√±os\n\n\n================================================================================\nFUNCI√ìN DE FORECAST PARA PRODUCCI√ìN\n================================================================================\n\nüîß Entrenando modelo FINAL para producci√≥n...\n   Total: Ridge\n   Proporciones: Ridge\n   Reconciliaci√≥n: ols\n\n‚úÖ Modelo entrenado con √©xito\n   Observaciones usadas: 30\n   Features Total: 5\n   Features Props: 11\n\n================================================================================\n‚úÖ IMPLEMENTACI√ìN COMPLETA FINALIZADA\n================================================================================\n\nüéØ MODELO FINAL ENTRENADO Y LISTO:\n   ‚Ä¢ Total: Ridge\n   ‚Ä¢ Proporciones: Ridge\n   ‚Ä¢ Reconciliaci√≥n: ols\n   ‚Ä¢ MAE esperado: 4.75 llantas\n   ‚Ä¢ Mejora vs TopDown: 0.0%\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluando: Total=Ridge, Props=Ridge\nEvaluando: Total=RandomForest, Props=Ridge\nEvaluando: Total=LightGBM, Props=Ridge\nEvaluando: Total=RandomForest, Props=RandomForest\nEvaluando: Total=LightGBM, Props=LightGBM"
  },
  {
    "objectID": "eda_final.html#variables-del-modelo-multi-output-de-predicci√≥n-semanal-de-consumo-de-llantas",
    "href": "eda_final.html#variables-del-modelo-multi-output-de-predicci√≥n-semanal-de-consumo-de-llantas",
    "title": "Informe",
    "section": "5.1 Variables del modelo multi-output de predicci√≥n semanal de consumo de llantas",
    "text": "5.1 Variables del modelo multi-output de predicci√≥n semanal de consumo de llantas\nEl modelo LSTM multi-output predice simult√°neamente el consumo total quincenal de llantas y su distribuci√≥n por tipo de cami√≥n.\n\n\n5.1.1 Variables Dependientes (Targets, multi-salida)\n\n\n\n\n\n\n\nVariable\nDescripci√≥n\n\n\n\n\ny_future_sum15_llantas_total\nSuma del consumo de llantas total esperado en los pr√≥ximos 7 d√≠as\n\n\ny_future_sum15_llantas_CAT789\nSuma del consumo semanal de llantas para camiones CAT789\n\n\ny_future_sum15_llantas_agrupadas_793_HIT4000\nSuma semanal para camiones CAT793 + HIT4000\n\n\ny_future_sum15_llantas_KOM930E\nSuma semanal para camiones KOM930E\n\n\n\n\nF√≥rmula general:\n[ y{(k)}t = {i=t+1}{t+7} ^{(k)}_i ] donde ( k ) representa cada categor√≠a de cami√≥n.\n\n\n\n\n5.1.2 Variables Ex√≥genas (Planificadas o conocidas en el d√≠a t)\nEstas provienen del plan minero o contexto operativo, y se conocen al momento de hacer la predicci√≥n.\n\n\n\nVariable\nDescripci√≥n\n\n\n\n\ntotal_payload\nCarga total transportada o planificada en el d√≠a\n\n\ntotal_loaded_kms\nKil√≥metros cargados planificados o esperados\n\n\nno_cycles\nN√∫mero de ciclos planificados o observados en el d√≠a\n\n\ntotal_cycle_time\nTiempo total de ciclo (promedio planificado)\n\n\navg_payload\nPeso promedio por ciclo\n\n\n\n\n‚úÖ Estas variables no se desplazan (shift), ya que est√°n disponibles en el d√≠a actual.\n\n\n\n\n5.1.3 Variables Dependientes del Pasado (Hist√≥ricas o End√≥genas)\nEstas reflejan el comportamiento hist√≥rico del consumo y la disponibilidad, y se calculan usando informaci√≥n pasada.\n\n\n\n\n\n\n\n\nVariable\nDescripci√≥n\nTipo\n\n\n\n\nllantas_total_lag1\nConsumo de llantas del d√≠a anterior\nLag\n\n\nllantas_total_lag7\nConsumo de llantas hace 7 d√≠as\nLag\n\n\nhoras_down_lag1\nHoras de inactividad del d√≠a anterior\nLag\n\n\nhoras_down_lag7\nHoras de inactividad hace 7 d√≠as\nLag\n\n\nllantas_total_sum7\nSuma del consumo en los √∫ltimos 7 d√≠as (tendencia corta)\nRolling\n\n\nllantas_total_mean7\nPromedio del consumo en los √∫ltimos 7 d√≠as\nRolling\n\n\nhoras_down_sum7\nSuma de horas inactivas en los √∫ltimos 7 d√≠as\nRolling\n\n\nhoras_down_mean7\nPromedio de horas inactivas en los √∫ltimos 7 d√≠as\nRolling\n\n\n\n\n‚öôÔ∏è Estas variables se construyen con .shift(1) antes del rolling() para evitar fuga temporal (data leakage).\n\n\n\n\n5.1.4 Variables Agregadas / Est√°ticas (Contexto resumido de los √∫ltimos 7 d√≠as)\nRepresentan el comportamiento agregado de las variables ex√≥genas recientes, usadas como entrada est√°tica (X_static).\n\n\n\n\n\n\n\nVariable\nDescripci√≥n\n\n\n\n\ntotal_payload_sum7 / total_payload_mean7\nCarga total acumulada y promedio en los √∫ltimos 7 d√≠as\n\n\ntotal_loaded_kms_sum7 / total_loaded_kms_mean7\nKil√≥metros cargados acumulados y promedio\n\n\nno_cycles_sum7 / no_cycles_mean7\nCiclos totales y promedio semanal\n\n\ntotal_cycle_time_sum7 / total_cycle_time_mean7\nTiempo total y promedio de ciclo semanal\n\n\navg_payload_sum7 / avg_payload_mean7\nSuma y promedio del payload medio diario\n\n\n\n\nEstas features forman el bloque de entrada est√°tica, combinadas con las din√°micas en el modelo.\n\n\n\n\n5.1.5 Estructura de entradas y salidas\n\n\n\n\n\n\n\n\n\nTipo\nNombre del bloque\nVariables incluidas\nDimensi√≥n\n\n\n\n\nEntrada secuencial (X_seq)\nSerie temporal de los √∫ltimos 30 d√≠as\nEx√≥genas planificadas + lags hist√≥ricos\n(batch, 30, n_features_seq)\n\n\nEntrada est√°tica (X_static)\nResumen del contexto semanal\nVariables _sum7 y _mean7\n(batch, n_features_static)\n\n\nSalida (Y multi-output)\nConsumo total y por tipo de llanta a 15 d√≠as\n4 variables (total, CAT789, CAT793+HIT4000, KOM930E)\n(batch, 4)\n\n\n\n\n\n\n5.1.6 Resumen General\n\n\n\n\n\n\n\n\n\nCategor√≠a\nVariables\nConocidas en t\nDescripci√≥n breve\n\n\n\n\nTargets (Y)\n4 (y_future_sum15_*)\n‚ùå No\nSumas futuras de consumo (15 d√≠as)\n\n\nEx√≥genas (planificadas)\ntotal_payload, total_loaded_kms, no_cycles, total_cycle_time, avg_payload\n‚úÖ S√≠\nContexto operativo del d√≠a\n\n\nHist√≥ricas (lags)\nllantas_total_lag1, llantas_total_lag7, horas_down_lag1, horas_down_lag7\n‚úÖ S√≠\nComportamiento pasado del sistema\n\n\nAgregadas (rolling 7d)\nTodas las _sum7 y _mean7\n‚úÖ S√≠\nContexto operativo reciente promedio\n\n\n\n\n\nEn resumen:\nEste modelo multi-output LSTM predice en una sola pasada el consumo total y por tipo de llanta para los pr√≥ximos 7 d√≠as,\ncombinando: - una ventana de 30 d√≠as de datos secuenciales (X_seq),\n- m√°s un conjunto de features agregadas est√°ticas (X_static).\nEsto permite capturar tanto la din√°mica temporal del consumo como la relaci√≥n estructural entre las distintas categor√≠as de cami√≥n."
  },
  {
    "objectID": "eda_final.html#exploratory-data-analysis",
    "href": "eda_final.html#exploratory-data-analysis",
    "title": "Informe",
    "section": "5.2 Exploratory Data Analysis",
    "text": "5.2 Exploratory Data Analysis\n\n\nüìä Registros obtenidos: 1,315\nüî¢ Columnas: 19\n\n============================================================\nüìã INFORMACI√ìN DEL DATASET\n============================================================\nShape: (1315, 19)\n\nPrimeras 3 filas:\n\n\n\n\n\n\n\n\n\nfecha\nllantas_CAT789\nllantas_CAT793\nllantas_HIT4000\nllantas_KOM930E\nllantas_total\nno_inspe_prio_1\nno_inspe_prio_2\nllantas_desechadas\nno_preventivos\nno_correctivos\navg_empty_speed\navg_payload\ntotal_empty_kms\ntotal_loaded_kms\ntotal_payload\nno_cycles\ntotal_cycle_time\nhoras_down\n\n\n\n\n0\n2021-01-02 00:00:00\n2.0\n4.0\n0.0\n0.0\n6\nNaN\nNaN\nNaN\nNone\nNone\n11.762016\n91.331976\n22423.720\n18693.891\n1056332.80\n4252.0\n2373.516\nNaN\n\n\n1\n2021-01-03 00:00:00\n2.0\n0.0\n0.0\n0.0\n2\nNaN\nNaN\nNaN\nNone\nNone\n11.125709\n95.595858\n21011.834\n18132.256\n987950.72\n4011.0\n2279.830\nNaN\n\n\n2\n2021-01-04 00:00:00\n0.0\n2.0\n0.0\n0.0\n2\nNaN\nNaN\nNaN\nNone\nNone\n10.806228\n93.053665\n24237.574\n21166.549\n1063807.31\n4249.0\n2636.066\nNaN\n\n\n\n\n\n\n\nDataset preparado:\nRango de fechas: 2021-01-02 00:00:00 a 2025-10-06 00:00:00\nN√∫mero de observaciones: 1315\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n================================================================================\nDETECCI√ìN DE OUTLIERS (M√©todo IQR)\n================================================================================\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n================================================================================\nRESUMEN DE OUTLIERS\n================================================================================\n\n\n\n\n\n\n\n\n\nNombre\nTotal_Datos\nOutliers\nPct_Outliers\n\n\n\n\n0\nLlantas Consumidas total\n1315\n3\n0.23\n\n\n1\nLlantas Consumidas CAT789\n1315\n15\n1.14\n\n\n2\nLlantas Consumidas CAT793 y HIT4000\n1315\n26\n1.98\n\n\n3\nLlantas Consumidas KOM930E\n1315\n309\n23.50\n\n\n4\nInspecciones Prio 1\n1315\n268\n20.38\n\n\n5\nInspecciones Prio 2\n1315\n199\n15.13\n\n\n6\nLlantas Desechadas\n1315\n10\n0.76\n\n\n7\nMant. Preventivos\n1315\n0\n0.00\n\n\n8\nMant. Correctivos\n1315\n0\n0.00\n\n\n9\nVel. Vac√≠o (km/h)\n1314\n48\n3.65\n\n\n10\nCarga Prom. (ton)\n1314\n15\n1.14\n\n\n11\nKm Vac√≠o Total\n1314\n77\n5.86\n\n\n12\nKm Cargado Total\n1314\n75\n5.71\n\n\n13\nCarga Total (ton)\n1314\n80\n6.09\n\n\n14\nN√∫mero de Ciclos\n1314\n98\n7.46\n\n\n15\nTiempo Total (hrs)\n1314\n83\n6.32\n\n\n16\nHoras down\n1315\n17\n1.29\n\n\n17\nPrecipitacion\n1315\n0\n0.00\n\n\n\n\n\n\n\n\n================================================================================\nRESUMEN EJECUTIVO\n================================================================================\n\nVariables con mayor % de outliers:\n  Llantas Consumidas KOM930E: 23.5% (309 casos)\n  Inspecciones Prio 1: 20.4% (268 casos)\n  Inspecciones Prio 2: 15.1% (199 casos)\n  N√∫mero de Ciclos: 7.5% (98 casos)\n  Tiempo Total (hrs): 6.3% (83 casos)\n\nVariables con distribuci√≥n m√°s normal:\n  Mant. Preventivos: 0.0% (0 casos)\n  Mant. Correctivos: 0.0% (0 casos)\n  Precipitacion: 0.0% (0 casos)\n  Llantas Consumidas total: 0.2% (3 casos)\n  Llantas Desechadas: 0.8% (10 casos)"
  },
  {
    "objectID": "eda_final.html#modelamiento-lstm",
    "href": "eda_final.html#modelamiento-lstm",
    "title": "Informe",
    "section": "5.3 Modelamiento LSTM",
    "text": "5.3 Modelamiento LSTM\n\n\nShapes:\nX_seq: (1270, 30, 9)\nX_static: (1270, 18)\ny: (1270, 4)\n\n\n\n\nModel: \"functional_1\"\n\n\n\n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ Layer (type)        ‚îÉ Output Shape      ‚îÉ    Param # ‚îÉ Connected to      ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ input_layer_2       ‚îÇ (None, 30, 9)     ‚îÇ          0 ‚îÇ -                 ‚îÇ\n‚îÇ (InputLayer)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ input_layer_3       ‚îÇ (None, 18)        ‚îÇ          0 ‚îÇ -                 ‚îÇ\n‚îÇ (InputLayer)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ lstm_1 (LSTM)       ‚îÇ (None, 64)        ‚îÇ     18,944 ‚îÇ input_layer_2[0]‚Ä¶ ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_4 (Dense)     ‚îÇ (None, 32)        ‚îÇ        608 ‚îÇ input_layer_3[0]‚Ä¶ ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dropout_2 (Dropout) ‚îÇ (None, 64)        ‚îÇ          0 ‚îÇ lstm_1[0][0]      ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dropout_3 (Dropout) ‚îÇ (None, 32)        ‚îÇ          0 ‚îÇ dense_4[0][0]     ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ concatenate_1       ‚îÇ (None, 96)        ‚îÇ          0 ‚îÇ dropout_2[0][0],  ‚îÇ\n‚îÇ (Concatenate)       ‚îÇ                   ‚îÇ            ‚îÇ dropout_3[0][0]   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_5 (Dense)     ‚îÇ (None, 64)        ‚îÇ      6,208 ‚îÇ concatenate_1[0]‚Ä¶ ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_6 (Dense)     ‚îÇ (None, 32)        ‚îÇ      2,080 ‚îÇ dense_5[0][0]     ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_7 (Dense)     ‚îÇ (None, 4)         ‚îÇ        132 ‚îÇ dense_6[0][0]     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n Total params: 27,972 (109.27 KB)\n\n\n\n Trainable params: 27,972 (109.27 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n\n\nEpoch 1/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 31s 1s/step - loss: 1.1895 - mae: 0.8331\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 1.1144 - mae: 0.8306\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 1.0434 - mae: 0.8063\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.9999 - mae: 0.7907\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 11ms/step - loss: 0.8641 - mae: 0.7395 - val_loss: 0.6184 - val_mae: 0.6385\n\nEpoch 2/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 22ms/step - loss: 0.6196 - mae: 0.6574\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.5670 - mae: 0.6049 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.5299 - mae: 0.5795\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.5020 - mae: 0.5616\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.4197 - mae: 0.5084 - val_loss: 0.2858 - val_mae: 0.4200\n\nEpoch 3/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 24ms/step - loss: 0.2048 - mae: 0.3524\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.2596 - mae: 0.4003 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.2569 - mae: 0.3988\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.2547 - mae: 0.3968\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.2464 - mae: 0.3889 - val_loss: 0.1922 - val_mae: 0.3308\n\nEpoch 4/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 22ms/step - loss: 0.2262 - mae: 0.3861\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.2143 - mae: 0.3640 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.2064 - mae: 0.3573\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.2017 - mae: 0.3525\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.1924 - mae: 0.3415 - val_loss: 0.1786 - val_mae: 0.3278\n\nEpoch 5/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 22ms/step - loss: 0.1221 - mae: 0.2801\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.1689 - mae: 0.3197 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.1724 - mae: 0.3225\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n29/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.1725 - mae: 0.3229\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 8ms/step - loss: 0.1709 - mae: 0.3219 - val_loss: 0.1232 - val_mae: 0.2646\n\nEpoch 6/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 21ms/step - loss: 0.1919 - mae: 0.3298\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.1820 - mae: 0.3306 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.1753 - mae: 0.3248\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.1704 - mae: 0.3200\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.1583 - mae: 0.3088 - val_loss: 0.1325 - val_mae: 0.2768\n\nEpoch 7/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 23ms/step - loss: 0.1246 - mae: 0.2679\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.1434 - mae: 0.2915 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.1437 - mae: 0.2925\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n28/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.1430 - mae: 0.2922\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 8ms/step - loss: 0.1379 - mae: 0.2885 - val_loss: 0.1269 - val_mae: 0.2733\n\nEpoch 8/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 23ms/step - loss: 0.1742 - mae: 0.2994\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.1481 - mae: 0.2917 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.1413 - mae: 0.2866\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.1370 - mae: 0.2827\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.1229 - mae: 0.2694 - val_loss: 0.1278 - val_mae: 0.2704\n\nEpoch 9/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 22ms/step - loss: 0.1305 - mae: 0.2883\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.1151 - mae: 0.2672 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.1126 - mae: 0.2622\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n27/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.1111 - mae: 0.2596\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 8ms/step - loss: 0.1095 - mae: 0.2537 - val_loss: 0.1107 - val_mae: 0.2468\n\nEpoch 10/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 21ms/step - loss: 0.0929 - mae: 0.2402\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0968 - mae: 0.2427 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.1025 - mae: 0.2476\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n31/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.1034 - mae: 0.2486\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.1032 - mae: 0.2478 - val_loss: 0.0997 - val_mae: 0.2396\n\nEpoch 11/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 21ms/step - loss: 0.0972 - mae: 0.2360\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.1002 - mae: 0.2427 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.1002 - mae: 0.2435\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.1001 - mae: 0.2438\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.0995 - mae: 0.2434 - val_loss: 0.0860 - val_mae: 0.2208\n\nEpoch 12/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 21ms/step - loss: 0.1309 - mae: 0.2688\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.1108 - mae: 0.2485 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.1062 - mae: 0.2451\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.1034 - mae: 0.2431\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.0954 - mae: 0.2367 - val_loss: 0.1033 - val_mae: 0.2370\n\nEpoch 13/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 21ms/step - loss: 0.1317 - mae: 0.2707\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0949 - mae: 0.2299 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0898 - mae: 0.2266\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n29/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0883 - mae: 0.2258\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 8ms/step - loss: 0.0855 - mae: 0.2246 - val_loss: 0.1004 - val_mae: 0.2428\n\nEpoch 14/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 22ms/step - loss: 0.0764 - mae: 0.2052\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0824 - mae: 0.2203 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0839 - mae: 0.2232\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0842 - mae: 0.2238\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.0834 - mae: 0.2240 - val_loss: 0.0873 - val_mae: 0.2223\n\nEpoch 15/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 22ms/step - loss: 0.0660 - mae: 0.2009\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0838 - mae: 0.2186 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0839 - mae: 0.2184\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n29/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0838 - mae: 0.2187\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.0847 - mae: 0.2206 - val_loss: 0.0709 - val_mae: 0.2050\n\nEpoch 16/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 23ms/step - loss: 0.0719 - mae: 0.2040\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 8ms/step - loss: 0.0780 - mae: 0.2186 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0809 - mae: 0.2232\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n27/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0807 - mae: 0.2230\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 8ms/step - loss: 0.0802 - mae: 0.2218 - val_loss: 0.1031 - val_mae: 0.2464\n\nEpoch 17/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 23ms/step - loss: 0.0612 - mae: 0.1983\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0754 - mae: 0.2121 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0785 - mae: 0.2154\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n28/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0791 - mae: 0.2160\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.0803 - mae: 0.2165 - val_loss: 0.0662 - val_mae: 0.1908\n\nEpoch 18/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 21ms/step - loss: 0.0809 - mae: 0.2203\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0783 - mae: 0.2188 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0787 - mae: 0.2180\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0783 - mae: 0.2167\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.0781 - mae: 0.2137 - val_loss: 0.0830 - val_mae: 0.2206\n\nEpoch 19/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 22ms/step - loss: 0.0526 - mae: 0.1809\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0692 - mae: 0.2015 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0707 - mae: 0.2043\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0713 - mae: 0.2051\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.0731 - mae: 0.2078 - val_loss: 0.0911 - val_mae: 0.2309\n\nEpoch 20/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 21ms/step - loss: 0.0989 - mae: 0.2354\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0780 - mae: 0.2116 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0738 - mae: 0.2061\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0721 - mae: 0.2039\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.0678 - mae: 0.1983 - val_loss: 0.0787 - val_mae: 0.2139\n\nEpoch 21/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 20ms/step - loss: 0.0695 - mae: 0.1985\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0719 - mae: 0.2065 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0716 - mae: 0.2052\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n28/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0708 - mae: 0.2035\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 8ms/step - loss: 0.0670 - mae: 0.1965 - val_loss: 0.0982 - val_mae: 0.2454\n\nEpoch 22/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 23ms/step - loss: 0.0597 - mae: 0.1956\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0703 - mae: 0.2008 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0702 - mae: 0.2005\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n31/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0695 - mae: 0.1999\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.0673 - mae: 0.1980 - val_loss: 0.0656 - val_mae: 0.1993\n\nEpoch 23/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 23ms/step - loss: 0.0634 - mae: 0.1932\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0661 - mae: 0.1962 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0648 - mae: 0.1938\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n31/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0644 - mae: 0.1929\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.0645 - mae: 0.1915 - val_loss: 0.0769 - val_mae: 0.2110\n\nEpoch 24/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 23ms/step - loss: 0.0753 - mae: 0.2001\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0671 - mae: 0.1929 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0676 - mae: 0.1945\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0676 - mae: 0.1950\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 8ms/step - loss: 0.0664 - mae: 0.1949 - val_loss: 0.0821 - val_mae: 0.2230\n\nEpoch 25/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 22ms/step - loss: 0.0649 - mae: 0.1833\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0710 - mae: 0.1970 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0696 - mae: 0.1978\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n28/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0691 - mae: 0.1982\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 8ms/step - loss: 0.0661 - mae: 0.1972 - val_loss: 0.0781 - val_mae: 0.2125\n\nEpoch 26/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 21ms/step - loss: 0.0543 - mae: 0.1799\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0606 - mae: 0.1842 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0616 - mae: 0.1853\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n31/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0619 - mae: 0.1863\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.0625 - mae: 0.1880 - val_loss: 0.0903 - val_mae: 0.2332\n\nEpoch 27/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 24ms/step - loss: 0.0814 - mae: 0.2147\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0692 - mae: 0.1971 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0660 - mae: 0.1934\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n31/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0641 - mae: 0.1909\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.0603 - mae: 0.1857 - val_loss: 0.0757 - val_mae: 0.2169\n\nEpoch 28/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 21ms/step - loss: 0.0414 - mae: 0.1641\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0509 - mae: 0.1749 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0557 - mae: 0.1814\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0576 - mae: 0.1838\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.0612 - mae: 0.1883 - val_loss: 0.0775 - val_mae: 0.2140\n\nEpoch 29/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 26ms/step - loss: 0.0628 - mae: 0.1846\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0631 - mae: 0.1857 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0631 - mae: 0.1862\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n29/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0620 - mae: 0.1853\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.0571 - mae: 0.1808 - val_loss: 0.0739 - val_mae: 0.2154\n\nEpoch 30/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 21ms/step - loss: 0.0490 - mae: 0.1710\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0583 - mae: 0.1781 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0578 - mae: 0.1786\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n31/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0573 - mae: 0.1784\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.0564 - mae: 0.1780 - val_loss: 0.0795 - val_mae: 0.2196\n\nEpoch 31/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 23ms/step - loss: 0.0875 - mae: 0.2100\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0613 - mae: 0.1812 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0576 - mae: 0.1785\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0569 - mae: 0.1789\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.0554 - mae: 0.1793 - val_loss: 0.0853 - val_mae: 0.2302\n\nEpoch 32/100\n\n\n 1/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 21ms/step - loss: 0.0334 - mae: 0.1407\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0482 - mae: 0.1608 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 6ms/step - loss: 0.0503 - mae: 0.1658\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - loss: 0.0508 - mae: 0.1676\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/32 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 7ms/step - loss: 0.0524 - mae: 0.1727 - val_loss: 0.0901 - val_mae: 0.2395\n\n\n\n\n\n\n\n1/2 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 16ms/step - loss: 0.0506 - mae: 0.1793\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n2/2 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 13ms/step - loss: 0.0652 - mae: 0.2015\n\n\n1/2 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 91ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n2/2 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 12ms/step\n\n\n\nüìä Resultados globales:\n\nMAE  : 1.351\n\nMSE  : 3.145\n\nSMAPE: 8.86%\n\nWAPE : 4.68%\n\n\n\n\n\n\n\n\n\n\n\n\nüìä M√©tricas por categor√≠a:\n\n\n\n\n\n\n\n\n\n\nVariable\nMAE\nMSE\nSMAPE (%)\nWAPE (%)\n\n\n\n\n0\nTotal\n1.803\n5.392\n3.371\n3.125\n\n\n1\nCAT789\n1.016\n1.447\n9.134\n8.014\n\n\n2\nCAT793+HIT4000\n1.494\n4.151\n4.347\n4.023\n\n\n3\nKOM930E\n1.093\n1.589\n18.593\n13.879\n\n\nGlobal (avg)\nPromedio Global\n1.351\n3.145\n8.862\n7.260"
  }
]